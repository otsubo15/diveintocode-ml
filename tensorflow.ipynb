{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 13　TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ライブラリのimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "%matplotlib inline\n",
    "# tensorflow1系\n",
    "# import tensorflow as tf\n",
    "# tensorflow2系\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 読み込み\n",
    "iris_dataset = load_iris()\n",
    "\n",
    "# 整形\n",
    "iris_dataframe = pd.DataFrame(data=iris_dataset.data, columns=iris_dataset.feature_names)\n",
    "iris_datalabel = pd.DataFrame(data=iris_dataset.target,columns=['Species'])\n",
    "df = pd.concat([iris_dataframe,iris_datalabel],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ミニバッチクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:#X, yの両方を入れる\n",
    "\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題2　スクラッチとTensorFlowの対応を考える"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:[[6.1 2.8 4.  1.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [6.  2.2 4.  1. ]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [6.7 3.  5.  1.7]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]]\n",
      "X_val:[[5.1 3.8 1.5 0.3]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [5.2 3.4 1.4 0.2]]\n",
      "y_train:[[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "y_val:[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "X_train正規化:[[0.02392157 0.01098039 0.01568627 0.00509804]\n",
      " [0.0172549  0.01254902 0.00509804 0.00078431]\n",
      " [0.02235294 0.01098039 0.01607843 0.00509804]\n",
      " [0.02       0.01372549 0.0054902  0.00117647]\n",
      " [0.02196078 0.01176471 0.01764706 0.00588235]\n",
      " [0.02       0.01294118 0.00666667 0.00196078]\n",
      " [0.02117647 0.01333333 0.00588235 0.00156863]\n",
      " [0.02       0.01490196 0.00745098 0.00156863]\n",
      " [0.02196078 0.00980392 0.01529412 0.00431373]\n",
      " [0.02470588 0.01294118 0.01843137 0.00627451]\n",
      " [0.01960784 0.01176471 0.00627451 0.00078431]\n",
      " [0.02156863 0.00941176 0.0145098  0.00392157]\n",
      " [0.01803922 0.01333333 0.0054902  0.00117647]\n",
      " [0.02117647 0.01333333 0.00666667 0.00078431]\n",
      " [0.02352941 0.01058824 0.02       0.00627451]\n",
      " [0.01882353 0.01176471 0.0054902  0.00117647]\n",
      " [0.01921569 0.01215686 0.00588235 0.00078431]\n",
      " [0.02431373 0.01137255 0.01686275 0.00509804]\n",
      " [0.0172549  0.01176471 0.00509804 0.00078431]\n",
      " [0.02156863 0.00941176 0.01490196 0.00431373]\n",
      " [0.02196078 0.01058824 0.01647059 0.00509804]\n",
      " [0.01764706 0.00901961 0.00509804 0.00117647]\n",
      " [0.01960784 0.00784314 0.01372549 0.00392157]\n",
      " [0.02039216 0.01058824 0.01529412 0.0054902 ]\n",
      " [0.01921569 0.00941176 0.01294118 0.00392157]\n",
      " [0.01921569 0.01411765 0.0054902  0.00039216]\n",
      " [0.02352941 0.00862745 0.01568627 0.00392157]\n",
      " [0.02       0.01372549 0.0054902  0.00078431]\n",
      " [0.02235294 0.01137255 0.01647059 0.00509804]\n",
      " [0.02509804 0.01254902 0.01764706 0.00588235]\n",
      " [0.02       0.00980392 0.01176471 0.00431373]\n",
      " [0.02509804 0.01137255 0.01686275 0.00509804]\n",
      " [0.02117647 0.01529412 0.00666667 0.00156863]\n",
      " [0.02156863 0.00980392 0.01568627 0.00509804]\n",
      " [0.01960784 0.01411765 0.0054902  0.00078431]\n",
      " [0.02745098 0.01254902 0.01843137 0.0054902 ]\n",
      " [0.02392157 0.01137255 0.01843137 0.0054902 ]\n",
      " [0.01960784 0.01372549 0.00627451 0.00235294]\n",
      " [0.01803922 0.01254902 0.0054902  0.00078431]\n",
      " [0.02039216 0.01372549 0.00588235 0.00078431]\n",
      " [0.02078431 0.0145098  0.00588235 0.00078431]\n",
      " [0.02627451 0.01176471 0.01960784 0.00666667]\n",
      " [0.02235294 0.01490196 0.00666667 0.00117647]\n",
      " [0.01843137 0.01254902 0.00627451 0.00078431]\n",
      " [0.01921569 0.01176471 0.0054902  0.00078431]\n",
      " [0.01921569 0.01215686 0.00588235 0.00039216]\n",
      " [0.0227451  0.01058824 0.01529412 0.00470588]\n",
      " [0.02431373 0.00862745 0.01764706 0.00588235]\n",
      " [0.02235294 0.0172549  0.00588235 0.00156863]\n",
      " [0.01960784 0.01254902 0.00470588 0.00078431]\n",
      " [0.02039216 0.01607843 0.00588235 0.00039216]\n",
      " [0.02705882 0.01215686 0.01921569 0.00588235]\n",
      " [0.02627451 0.01215686 0.0172549  0.0054902 ]\n",
      " [0.02470588 0.00980392 0.01921569 0.00588235]\n",
      " [0.02156863 0.01019608 0.0172549  0.00470588]\n",
      " [0.02470588 0.00901961 0.0172549  0.00509804]\n",
      " [0.01882353 0.01333333 0.00627451 0.00078431]\n",
      " [0.02313725 0.01176471 0.01647059 0.00588235]\n",
      " [0.02666667 0.01098039 0.01882353 0.0054902 ]\n",
      " [0.0227451  0.01058824 0.01607843 0.00392157]\n",
      " [0.01882353 0.01176471 0.0054902  0.00039216]\n",
      " [0.02       0.01333333 0.00588235 0.00078431]\n",
      " [0.02117647 0.01176471 0.01764706 0.00588235]\n",
      " [0.02235294 0.01019608 0.01372549 0.00392157]]\n",
      "X_test正規化:[[0.01960784 0.01333333 0.00627451 0.00156863]\n",
      " [0.02627451 0.01215686 0.01843137 0.00588235]\n",
      " [0.01843137 0.01254902 0.00509804 0.00078431]\n",
      " [0.02235294 0.01098039 0.01764706 0.00509804]\n",
      " [0.02588235 0.01176471 0.0172549  0.0054902 ]\n",
      " [0.01960784 0.00901961 0.01294118 0.00392157]\n",
      " [0.02117647 0.01529412 0.00509804 0.00156863]\n",
      " [0.02392157 0.01098039 0.01843137 0.00470588]\n",
      " [0.0254902  0.01098039 0.01803922 0.00588235]\n",
      " [0.02235294 0.01176471 0.01647059 0.00470588]\n",
      " [0.02156863 0.00901961 0.01568627 0.00509804]\n",
      " [0.0227451  0.01019608 0.01568627 0.00470588]\n",
      " [0.02352941 0.01137255 0.01764706 0.00588235]\n",
      " [0.01686275 0.01176471 0.00431373 0.00039216]\n",
      " [0.01960784 0.01333333 0.00588235 0.00078431]\n",
      " [0.01882353 0.01215686 0.00627451 0.00078431]\n",
      " [0.01803922 0.01411765 0.00392157 0.00078431]\n",
      " [0.01882353 0.01333333 0.00745098 0.00078431]\n",
      " [0.02156863 0.01647059 0.0054902  0.00078431]\n",
      " [0.0172549  0.01137255 0.0054902  0.00078431]]\n",
      "X_val正規化:[[0.02       0.01490196 0.00588235 0.00117647]\n",
      " [0.01960784 0.01372549 0.00509804 0.00117647]\n",
      " [0.02392157 0.01176471 0.01803922 0.0054902 ]\n",
      " [0.02196078 0.01137255 0.01411765 0.00509804]\n",
      " [0.02352941 0.01333333 0.01764706 0.00627451]\n",
      " [0.02156863 0.01372549 0.00509804 0.00078431]\n",
      " [0.02588235 0.01137255 0.01803922 0.00509804]\n",
      " [0.02196078 0.01176471 0.01607843 0.00509804]\n",
      " [0.0227451  0.01568627 0.00470588 0.00078431]\n",
      " [0.02313725 0.01254902 0.01882353 0.00705882]\n",
      " [0.02117647 0.0145098  0.00588235 0.00078431]\n",
      " [0.02       0.01490196 0.00627451 0.00078431]\n",
      " [0.01803922 0.01215686 0.00588235 0.00078431]\n",
      " [0.02       0.0145098  0.00588235 0.00156863]\n",
      " [0.01960784 0.01294118 0.0054902  0.00078431]\n",
      " [0.02039216 0.01333333 0.0054902  0.00078431]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\new\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# 2値分類のため絞り込み\n",
    "df2 = df[(df[\"Species\"] == 0)|(df[\"Species\"] == 1)]\n",
    "\n",
    "# 説明変数と目的変数に分割\n",
    "y = df2[\"Species\"]\n",
    "X = df2.loc[:, [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# 訓練データ/テストデータ/評価データに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "print(\"X_train:{}\".format(X_train)) #------検証中--------\n",
    "print(\"X_val:{}\".format(X_val)) #------検証中--------\n",
    "print(\"y_train:{}\".format(y_train)) #------検証中--------\n",
    "print(\"y_val:{}\".format(y_val)) #------検証中--------\n",
    "\n",
    "# 正規化\n",
    "X_train /= 255#★☆★☆これは何を表してるのか謎\n",
    "X_test /= 255\n",
    "X_val /= 255\n",
    "print(\"X_train正規化:{}\".format(X_train)) #------検証中--------\n",
    "print(\"X_test正規化:{}\".format(X_test)) #------検証中--------\n",
    "print(\"X_val正規化:{}\".format(X_val)) #------検証中--------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflowで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Owner\\anaconda3\\envs\\new\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\new\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_loss : 0.9474, val_loss : 0.6882, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 1, train_loss : 1.4292, val_loss : 0.9889, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 2, train_loss : 0.7078, val_loss : 0.5569, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 3, train_loss : 0.5957, val_loss : 0.5214, train_acc : 0.594, val_acc : 0.812\n",
      "Epoch 4, train_loss : 0.6973, val_loss : 0.5357, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 5, train_loss : 0.8710, val_loss : 0.6199, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 6, train_loss : 0.7657, val_loss : 0.5553, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 7, train_loss : 0.6617, val_loss : 0.4963, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 8, train_loss : 0.6621, val_loss : 0.4893, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 9, train_loss : 0.6988, val_loss : 0.5018, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 10, train_loss : 0.6919, val_loss : 0.4923, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 11, train_loss : 0.6489, val_loss : 0.4635, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 12, train_loss : 0.6168, val_loss : 0.4407, train_acc : 0.484, val_acc : 0.625\n",
      "Epoch 13, train_loss : 0.6032, val_loss : 0.4277, train_acc : 0.500, val_acc : 0.625\n",
      "Epoch 14, train_loss : 0.5928, val_loss : 0.4165, train_acc : 0.516, val_acc : 0.625\n",
      "Epoch 15, train_loss : 0.5744, val_loss : 0.4012, train_acc : 0.531, val_acc : 0.625\n",
      "Epoch 16, train_loss : 0.5502, val_loss : 0.3829, train_acc : 0.578, val_acc : 0.688\n",
      "Epoch 17, train_loss : 0.5262, val_loss : 0.3649, train_acc : 0.609, val_acc : 0.812\n",
      "Epoch 18, train_loss : 0.5047, val_loss : 0.3483, train_acc : 0.625, val_acc : 0.875\n",
      "Epoch 19, train_loss : 0.4844, val_loss : 0.3325, train_acc : 0.656, val_acc : 0.875\n",
      "Epoch 20, train_loss : 0.4635, val_loss : 0.3166, train_acc : 0.703, val_acc : 0.875\n",
      "Epoch 21, train_loss : 0.4416, val_loss : 0.3004, train_acc : 0.734, val_acc : 0.875\n",
      "Epoch 22, train_loss : 0.4192, val_loss : 0.2841, train_acc : 0.734, val_acc : 0.875\n",
      "Epoch 23, train_loss : 0.3969, val_loss : 0.2682, train_acc : 0.797, val_acc : 0.875\n",
      "Epoch 24, train_loss : 0.3751, val_loss : 0.2529, train_acc : 0.812, val_acc : 0.938\n",
      "Epoch 25, train_loss : 0.3539, val_loss : 0.2381, train_acc : 0.844, val_acc : 0.938\n",
      "Epoch 26, train_loss : 0.3332, val_loss : 0.2239, train_acc : 0.891, val_acc : 0.938\n",
      "Epoch 27, train_loss : 0.3133, val_loss : 0.2105, train_acc : 0.906, val_acc : 0.938\n",
      "Epoch 28, train_loss : 0.2943, val_loss : 0.1979, train_acc : 0.922, val_acc : 1.000\n",
      "Epoch 29, train_loss : 0.2764, val_loss : 0.1861, train_acc : 0.922, val_acc : 1.000\n",
      "Epoch 30, train_loss : 0.2597, val_loss : 0.1752, train_acc : 0.938, val_acc : 1.000\n",
      "Epoch 31, train_loss : 0.2443, val_loss : 0.1653, train_acc : 0.953, val_acc : 1.000\n",
      "Epoch 32, train_loss : 0.2302, val_loss : 0.1561, train_acc : 0.953, val_acc : 1.000\n",
      "Epoch 33, train_loss : 0.2172, val_loss : 0.1477, train_acc : 0.953, val_acc : 1.000\n",
      "Epoch 34, train_loss : 0.2052, val_loss : 0.1400, train_acc : 0.953, val_acc : 1.000\n",
      "Epoch 35, train_loss : 0.1942, val_loss : 0.1330, train_acc : 0.953, val_acc : 1.000\n",
      "Epoch 36, train_loss : 0.1842, val_loss : 0.1264, train_acc : 0.969, val_acc : 1.000\n",
      "Epoch 37, train_loss : 0.1749, val_loss : 0.1204, train_acc : 0.969, val_acc : 1.000\n",
      "Epoch 38, train_loss : 0.1663, val_loss : 0.1148, train_acc : 0.969, val_acc : 1.000\n",
      "Epoch 39, train_loss : 0.1584, val_loss : 0.1096, train_acc : 0.984, val_acc : 1.000\n",
      "Epoch 40, train_loss : 0.1510, val_loss : 0.1048, train_acc : 0.984, val_acc : 1.000\n",
      "Epoch 41, train_loss : 0.1442, val_loss : 0.1002, train_acc : 0.984, val_acc : 1.000\n",
      "Epoch 42, train_loss : 0.1379, val_loss : 0.0960, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 43, train_loss : 0.1319, val_loss : 0.0920, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 44, train_loss : 0.1264, val_loss : 0.0882, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 45, train_loss : 0.1212, val_loss : 0.0847, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 46, train_loss : 0.1164, val_loss : 0.0813, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 47, train_loss : 0.1117, val_loss : 0.0782, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 48, train_loss : 0.1074, val_loss : 0.0752, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 49, train_loss : 0.1034, val_loss : 0.0723, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 50, train_loss : 0.0995, val_loss : 0.0697, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 51, train_loss : 0.0959, val_loss : 0.0671, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 52, train_loss : 0.0925, val_loss : 0.0647, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 53, train_loss : 0.0892, val_loss : 0.0623, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 54, train_loss : 0.0861, val_loss : 0.0601, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 55, train_loss : 0.0832, val_loss : 0.0580, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 56, train_loss : 0.0804, val_loss : 0.0560, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 57, train_loss : 0.0777, val_loss : 0.0541, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 58, train_loss : 0.0752, val_loss : 0.0522, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 59, train_loss : 0.0727, val_loss : 0.0505, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 60, train_loss : 0.0704, val_loss : 0.0488, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 61, train_loss : 0.0682, val_loss : 0.0472, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 62, train_loss : 0.0661, val_loss : 0.0456, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 63, train_loss : 0.0641, val_loss : 0.0442, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 64, train_loss : 0.0622, val_loss : 0.0428, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 65, train_loss : 0.0603, val_loss : 0.0414, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 66, train_loss : 0.0585, val_loss : 0.0401, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 67, train_loss : 0.0568, val_loss : 0.0388, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 68, train_loss : 0.0552, val_loss : 0.0376, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 69, train_loss : 0.0536, val_loss : 0.0365, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 70, train_loss : 0.0521, val_loss : 0.0354, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 71, train_loss : 0.0507, val_loss : 0.0343, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 72, train_loss : 0.0493, val_loss : 0.0333, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 73, train_loss : 0.0479, val_loss : 0.0323, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 74, train_loss : 0.0466, val_loss : 0.0313, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 75, train_loss : 0.0454, val_loss : 0.0304, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 76, train_loss : 0.0442, val_loss : 0.0295, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 77, train_loss : 0.0430, val_loss : 0.0287, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 78, train_loss : 0.0419, val_loss : 0.0278, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 79, train_loss : 0.0409, val_loss : 0.0271, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 80, train_loss : 0.0398, val_loss : 0.0263, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 81, train_loss : 0.0388, val_loss : 0.0256, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 82, train_loss : 0.0379, val_loss : 0.0249, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 83, train_loss : 0.0369, val_loss : 0.0242, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 84, train_loss : 0.0360, val_loss : 0.0235, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 85, train_loss : 0.0352, val_loss : 0.0229, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 86, train_loss : 0.0343, val_loss : 0.0223, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 87, train_loss : 0.0335, val_loss : 0.0217, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 88, train_loss : 0.0327, val_loss : 0.0211, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 89, train_loss : 0.0320, val_loss : 0.0205, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 90, train_loss : 0.0312, val_loss : 0.0200, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 91, train_loss : 0.0305, val_loss : 0.0195, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 92, train_loss : 0.0298, val_loss : 0.0190, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 93, train_loss : 0.0292, val_loss : 0.0185, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 94, train_loss : 0.0285, val_loss : 0.0180, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 95, train_loss : 0.0279, val_loss : 0.0176, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 96, train_loss : 0.0273, val_loss : 0.0172, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 97, train_loss : 0.0267, val_loss : 0.0167, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 98, train_loss : 0.0261, val_loss : 0.0163, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 99, train_loss : 0.0256, val_loss : 0.0159, train_acc : 1.000, val_acc : 1.000\n",
      "test_acc : 1.000\n"
     ]
    }
   ],
   "source": [
    "# 各種変数定義\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "# 空配列定義\n",
    "X = tf.placeholder(\"float\", [None, n_input])#n_inputはX_trainの特徴量の数\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])#n_classesは\n",
    "\n",
    "# ミニバッチイテレータ生成\n",
    "# インスタンス化 get_mini_batch_trainにGetMiniBatchのリターンのX[p0:p1]＆y[p0:p1]が入ってくる。\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"tensorflowを利用したニューラルネットワーク\n",
    "    Parameters\n",
    "    ---------------\n",
    "    x : 入力配列\n",
    "    \"\"\"\n",
    "    # 重み定義#なんで３つと決めれた？入力層と出力層を除いた層の数自分で決めていい？何が違う？\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    # バイアス定義\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    # 計算グラフ構築（順伝播処理）\n",
    "\n",
    "    #第１層の内積とバイアスはw１ * X + b１ で求められる。\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    #ＬｅＬＵにlayer_1を代入\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "    #第２層の内積とバイアスはw２ * X + b２ で求められる。\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    #ＬｅＬＵにlayer_２を代入\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "\n",
    "    #\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
    "    return layer_output\n",
    "\n",
    "# 計算グラフ受け取る\n",
    "logits = example_net(X)\n",
    "\n",
    "\n",
    "\n",
    "# 損失定義\n",
    "## tf.reduce_mean：与えたリストに入っている数値の平均値を求める関数\n",
    "###★☆★☆tf.nn.sigmoid_cross_entropy_with_logitsの(labels=Y, logits=logits)は何でどうなる？\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))#labels=Y, logits=logitsは何？\n",
    "\n",
    "\n",
    "\n",
    "# 最適化手法の定義（勾配降下法みたいなものパラメータの最適化を行う）\n",
    "## tf.train.AdamOptimizerの使い方：初期値として学習率を指定し、値を最小化するため「minimize()」関数に損失関数を渡す。\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "# 最適化手法で、定義した損失を最小化するルールを作成\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "\n",
    "\n",
    "# ACC計算\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "# 変数を扱うためのおまじない\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "\n",
    "# tensorflowのセッション開始\n",
    "with tf.Session() as sess:\n",
    "    # 初期化\n",
    "    sess.run(init)\n",
    "    # 学習回数分ループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # ミニバッチイテレータでループ\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # 「最適化手法で、定義した損失を最小化するルール」を実行\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "        # 記録\n",
    "        train_loss, train_acc = sess.run([loss_op, accuracy], feed_dict={X: X_train, Y: y_train})\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        # 仮定出力\n",
    "        print(\"Epoch {}, train_loss : {:.4f}, val_loss : {:.4f}, train_acc : {:.3f}, val_acc : {:.3f}\".format(epoch, train_loss, val_loss, train_acc, val_acc))\n",
    "    \n",
    "    # 学習が終了したらテストデータで実行\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    #prediction = sess.run(logits, feed_dict={X: X_test, Y: y_test})\n",
    "    #print(prediction)\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題3　3種類すべての目的変数を使用したIrisのモデルを作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\new\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 説明変数と目的変数に分割\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# 訓練データ/テストデータ/評価データに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# onehotベクトル化\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train)\n",
    "y_val_one_hot = enc.transform(y_val)\n",
    "y_test_one_hot = enc.transform(y_test)\n",
    "\n",
    "# 正規化\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_val /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflowで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\new\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_loss : 2.7265, val_loss : 2.6540, train_acc : 0.323, val_acc : 0.333\n",
      "Epoch 1, train_loss : 2.5269, val_loss : 2.4728, train_acc : 0.323, val_acc : 0.333\n",
      "Epoch 2, train_loss : 2.1484, val_loss : 2.0570, train_acc : 0.323, val_acc : 0.333\n",
      "Epoch 3, train_loss : 2.2941, val_loss : 2.2640, train_acc : 0.323, val_acc : 0.333\n",
      "Epoch 4, train_loss : 2.0610, val_loss : 2.0546, train_acc : 0.365, val_acc : 0.333\n",
      "Epoch 5, train_loss : 1.4613, val_loss : 1.4399, train_acc : 0.417, val_acc : 0.500\n",
      "Epoch 6, train_loss : 0.9523, val_loss : 0.9100, train_acc : 0.354, val_acc : 0.333\n",
      "Epoch 7, train_loss : 1.3355, val_loss : 1.3252, train_acc : 0.604, val_acc : 0.625\n",
      "Epoch 8, train_loss : 1.0278, val_loss : 1.0178, train_acc : 0.490, val_acc : 0.583\n",
      "Epoch 9, train_loss : 0.8405, val_loss : 0.8240, train_acc : 0.615, val_acc : 0.583\n",
      "Epoch 10, train_loss : 1.0391, val_loss : 1.0321, train_acc : 0.375, val_acc : 0.417\n",
      "Epoch 11, train_loss : 0.8184, val_loss : 0.8051, train_acc : 0.531, val_acc : 0.583\n",
      "Epoch 12, train_loss : 0.8911, val_loss : 0.8786, train_acc : 0.490, val_acc : 0.542\n",
      "Epoch 13, train_loss : 0.7903, val_loss : 0.7763, train_acc : 0.573, val_acc : 0.583\n",
      "Epoch 14, train_loss : 0.8259, val_loss : 0.8137, train_acc : 0.469, val_acc : 0.542\n",
      "Epoch 15, train_loss : 0.7730, val_loss : 0.7614, train_acc : 0.500, val_acc : 0.542\n",
      "Epoch 16, train_loss : 0.7821, val_loss : 0.7708, train_acc : 0.438, val_acc : 0.542\n",
      "Epoch 17, train_loss : 0.7463, val_loss : 0.7351, train_acc : 0.510, val_acc : 0.583\n",
      "Epoch 18, train_loss : 0.7387, val_loss : 0.7270, train_acc : 0.510, val_acc : 0.583\n",
      "Epoch 19, train_loss : 0.7108, val_loss : 0.6988, train_acc : 0.583, val_acc : 0.667\n",
      "Epoch 20, train_loss : 0.6951, val_loss : 0.6825, train_acc : 0.604, val_acc : 0.667\n",
      "Epoch 21, train_loss : 0.6698, val_loss : 0.6569, train_acc : 0.656, val_acc : 0.667\n",
      "Epoch 22, train_loss : 0.6491, val_loss : 0.6356, train_acc : 0.677, val_acc : 0.667\n",
      "Epoch 23, train_loss : 0.6239, val_loss : 0.6101, train_acc : 0.708, val_acc : 0.708\n",
      "Epoch 24, train_loss : 0.6005, val_loss : 0.5865, train_acc : 0.750, val_acc : 0.750\n",
      "Epoch 25, train_loss : 0.5757, val_loss : 0.5616, train_acc : 0.771, val_acc : 0.792\n",
      "Epoch 26, train_loss : 0.5525, val_loss : 0.5384, train_acc : 0.823, val_acc : 0.833\n",
      "Epoch 27, train_loss : 0.5300, val_loss : 0.5161, train_acc : 0.833, val_acc : 0.833\n",
      "Epoch 28, train_loss : 0.5095, val_loss : 0.4960, train_acc : 0.854, val_acc : 0.833\n",
      "Epoch 29, train_loss : 0.4905, val_loss : 0.4776, train_acc : 0.854, val_acc : 0.833\n",
      "Epoch 30, train_loss : 0.4735, val_loss : 0.4614, train_acc : 0.875, val_acc : 0.875\n",
      "Epoch 31, train_loss : 0.4582, val_loss : 0.4468, train_acc : 0.885, val_acc : 0.917\n",
      "Epoch 32, train_loss : 0.4445, val_loss : 0.4341, train_acc : 0.896, val_acc : 0.917\n",
      "Epoch 33, train_loss : 0.4321, val_loss : 0.4227, train_acc : 0.906, val_acc : 0.917\n",
      "Epoch 34, train_loss : 0.4208, val_loss : 0.4124, train_acc : 0.896, val_acc : 0.917\n",
      "Epoch 35, train_loss : 0.4106, val_loss : 0.4032, train_acc : 0.896, val_acc : 0.917\n",
      "Epoch 36, train_loss : 0.4010, val_loss : 0.3947, train_acc : 0.896, val_acc : 0.917\n",
      "Epoch 37, train_loss : 0.3922, val_loss : 0.3869, train_acc : 0.885, val_acc : 0.917\n",
      "Epoch 38, train_loss : 0.3839, val_loss : 0.3796, train_acc : 0.885, val_acc : 0.917\n",
      "Epoch 39, train_loss : 0.3760, val_loss : 0.3728, train_acc : 0.896, val_acc : 0.917\n",
      "Epoch 40, train_loss : 0.3686, val_loss : 0.3665, train_acc : 0.896, val_acc : 0.917\n",
      "Epoch 41, train_loss : 0.3616, val_loss : 0.3605, train_acc : 0.896, val_acc : 0.917\n",
      "Epoch 42, train_loss : 0.3548, val_loss : 0.3548, train_acc : 0.917, val_acc : 0.958\n",
      "Epoch 43, train_loss : 0.3483, val_loss : 0.3493, train_acc : 0.906, val_acc : 0.958\n",
      "Epoch 44, train_loss : 0.3421, val_loss : 0.3441, train_acc : 0.927, val_acc : 0.958\n",
      "Epoch 45, train_loss : 0.3361, val_loss : 0.3391, train_acc : 0.927, val_acc : 0.958\n",
      "Epoch 46, train_loss : 0.3303, val_loss : 0.3343, train_acc : 0.927, val_acc : 0.958\n",
      "Epoch 47, train_loss : 0.3247, val_loss : 0.3298, train_acc : 0.938, val_acc : 0.917\n",
      "Epoch 48, train_loss : 0.3191, val_loss : 0.3251, train_acc : 0.938, val_acc : 0.958\n",
      "Epoch 49, train_loss : 0.3141, val_loss : 0.3211, train_acc : 0.938, val_acc : 0.958\n",
      "Epoch 50, train_loss : 0.3088, val_loss : 0.3169, train_acc : 0.938, val_acc : 0.958\n",
      "Epoch 51, train_loss : 0.3037, val_loss : 0.3129, train_acc : 0.938, val_acc : 0.958\n",
      "Epoch 52, train_loss : 0.2990, val_loss : 0.3093, train_acc : 0.938, val_acc : 0.917\n",
      "Epoch 53, train_loss : 0.2941, val_loss : 0.3055, train_acc : 0.938, val_acc : 0.917\n",
      "Epoch 54, train_loss : 0.2896, val_loss : 0.3021, train_acc : 0.938, val_acc : 0.917\n",
      "Epoch 55, train_loss : 0.2851, val_loss : 0.2987, train_acc : 0.948, val_acc : 0.917\n",
      "Epoch 56, train_loss : 0.2808, val_loss : 0.2954, train_acc : 0.948, val_acc : 0.917\n",
      "Epoch 57, train_loss : 0.2765, val_loss : 0.2923, train_acc : 0.948, val_acc : 0.917\n",
      "Epoch 58, train_loss : 0.2724, val_loss : 0.2891, train_acc : 0.948, val_acc : 0.917\n",
      "Epoch 59, train_loss : 0.2682, val_loss : 0.2860, train_acc : 0.948, val_acc : 0.917\n",
      "Epoch 60, train_loss : 0.2644, val_loss : 0.2830, train_acc : 0.948, val_acc : 0.917\n",
      "Epoch 61, train_loss : 0.2605, val_loss : 0.2804, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 62, train_loss : 0.2568, val_loss : 0.2774, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 63, train_loss : 0.2531, val_loss : 0.2747, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 64, train_loss : 0.2495, val_loss : 0.2723, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 65, train_loss : 0.2460, val_loss : 0.2699, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 66, train_loss : 0.2427, val_loss : 0.2672, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 67, train_loss : 0.2394, val_loss : 0.2651, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 68, train_loss : 0.2362, val_loss : 0.2635, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 69, train_loss : 0.2331, val_loss : 0.2606, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 70, train_loss : 0.2300, val_loss : 0.2584, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 71, train_loss : 0.2269, val_loss : 0.2570, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 72, train_loss : 0.2240, val_loss : 0.2547, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 73, train_loss : 0.2212, val_loss : 0.2527, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 74, train_loss : 0.2184, val_loss : 0.2508, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 75, train_loss : 0.2157, val_loss : 0.2497, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 76, train_loss : 0.2130, val_loss : 0.2472, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 77, train_loss : 0.2105, val_loss : 0.2461, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 78, train_loss : 0.2079, val_loss : 0.2444, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 79, train_loss : 0.2055, val_loss : 0.2429, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 80, train_loss : 0.2031, val_loss : 0.2415, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 81, train_loss : 0.2009, val_loss : 0.2404, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 82, train_loss : 0.1985, val_loss : 0.2384, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 83, train_loss : 0.1965, val_loss : 0.2380, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 84, train_loss : 0.1941, val_loss : 0.2358, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 85, train_loss : 0.1924, val_loss : 0.2357, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 86, train_loss : 0.1900, val_loss : 0.2329, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 87, train_loss : 0.1891, val_loss : 0.2351, train_acc : 0.938, val_acc : 0.875\n",
      "Epoch 88, train_loss : 0.1854, val_loss : 0.2277, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 89, train_loss : 0.1859, val_loss : 0.2354, train_acc : 0.938, val_acc : 0.875\n",
      "Epoch 90, train_loss : 0.1818, val_loss : 0.2254, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 91, train_loss : 0.1831, val_loss : 0.2347, train_acc : 0.938, val_acc : 0.875\n",
      "Epoch 92, train_loss : 0.1783, val_loss : 0.2207, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 93, train_loss : 0.1808, val_loss : 0.2353, train_acc : 0.938, val_acc : 0.875\n",
      "Epoch 94, train_loss : 0.1742, val_loss : 0.2173, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 95, train_loss : 0.1786, val_loss : 0.2371, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 96, train_loss : 0.1710, val_loss : 0.2151, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 97, train_loss : 0.1763, val_loss : 0.2367, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 98, train_loss : 0.1678, val_loss : 0.2119, train_acc : 0.948, val_acc : 0.875\n",
      "Epoch 99, train_loss : 0.1750, val_loss : 0.2383, train_acc : 0.948, val_acc : 0.833\n",
      "test_acc : 0.900\n"
     ]
    }
   ],
   "source": [
    "# 各種変数定義\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 3 # 2値分類からの変更箇所\n",
    "\n",
    "# 空配列定義\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# ミニバッチイテレータ生成\n",
    "## インスタンス化 GetMiniBatchにリターンのX[p0:p1]＆y[p0:p1]が入っている。\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"tensorflowを利用したニューラルネットワーク\n",
    "    Parameters\n",
    "    ---------------\n",
    "    x : 入力配列\n",
    "    \"\"\"\n",
    "    # 重み定義\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    # バイアス定義\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    # 計算グラフ構築（順伝播処理）\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
    "    return layer_output\n",
    "\n",
    "# 計算グラフ受け取る\n",
    "logits = example_net(X)\n",
    "# 損失定義\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits)) # 2値分類からの変更箇所\n",
    "# 最適化手法の定義\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "# 最適化手法で、定義した損失を最小化するルールを作成\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# ACC計算\n",
    "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1)) # 2値分類からの変更箇所\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# 変数を扱うためのおまじない\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# tensorflowのセッション開始\n",
    "with tf.Session() as sess:\n",
    "    # 初期化\n",
    "    sess.run(init)\n",
    "    # 学習回数分ループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # ミニバッチイテレータでループ\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # 「最適化手法で、定義した損失を最小化するルール」を実行\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "        # 記録\n",
    "        train_loss, train_acc = sess.run([loss_op, accuracy], feed_dict={X: X_train, Y: y_train_one_hot})\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val_one_hot})\n",
    "        # 仮定出力\n",
    "        print(\"Epoch {}, train_loss : {:.4f}, val_loss : {:.4f}, train_acc : {:.3f}, val_acc : {:.3f}\".format(epoch, train_loss, val_loss, train_acc, val_acc))\n",
    "    \n",
    "    # 学習が終了したらテストデータで実行\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
    "    #prediction = sess.run(logits, feed_dict={X: X_test, Y: y_test})\n",
    "    #print(prediction)\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題4　House Pricesのモデルを作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\new\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dataset_path =\"train.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "y = df[\"SalePrice\"]\n",
    "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "y = np.log(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflowで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\new\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 5906825.5000, val_loss : 7989858.0000\n",
      "Epoch 1, loss : 1665153.8750, val_loss : 1736107.7500\n",
      "Epoch 2, loss : 987193.3125, val_loss : 834178.5000\n",
      "Epoch 3, loss : 669414.0000, val_loss : 546524.0625\n",
      "Epoch 4, loss : 484437.8125, val_loss : 427907.2188\n",
      "Epoch 5, loss : 359385.5312, val_loss : 359361.3750\n",
      "Epoch 6, loss : 267373.3125, val_loss : 303999.8438\n",
      "Epoch 7, loss : 204910.6562, val_loss : 258346.3281\n",
      "Epoch 8, loss : 159016.0000, val_loss : 215073.6250\n",
      "Epoch 9, loss : 122737.8984, val_loss : 173614.9219\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mse : 799220.438\n"
     ]
    }
   ],
   "source": [
    "# 各種変数定義\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "# 空配列定義\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# ミニバッチイテレータ生成\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"tensorflowを利用したニューラルネットワーク\n",
    "    Parameters\n",
    "    ---------------\n",
    "    x : 入力配列\n",
    "    \"\"\"\n",
    "    # 重み定義\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    # バイアス定義\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    # 計算グラフ構築（順伝播処理）\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
    "    return layer_output\n",
    "\n",
    "# 計算グラフ受け取る\n",
    "logits = example_net(X)\n",
    "# 損失定義\n",
    "loss_op =  tf.losses.mean_squared_error(labels=Y, predictions=logits)\n",
    "# 最適化手法の定義\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "# 最適化手法で、定義した損失を最小化するルールを作成\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 変数を扱うためのおまじない\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# tensorflowのセッション開始\n",
    "with tf.Session() as sess:\n",
    "    # 初期化\n",
    "    sess.run(init)\n",
    "    # 損失記録用リスト\n",
    "    loss_list = []\n",
    "    val_loss_list = []\n",
    "    # 学習回数分ループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # ミニバッチイテレータでループ\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # 「最適化手法で、定義した損失を最小化するルール」を実行\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "        # 損失計算と格納\n",
    "        loss = sess.run(loss_op, feed_dict={X: X_train, Y: y_train})\n",
    "        val_loss = sess.run(loss_op, feed_dict={X: X_val, Y: y_val})\n",
    "        loss_list.append(loss)\n",
    "        val_loss_list.append(val_loss)    \n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}\".format(epoch, loss, val_loss))\n",
    "    \n",
    "    # 学習過程可視化\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.plot(loss_list, label='loss')\n",
    "    plt.plot(val_loss_list, label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # テストデータに適用\n",
    "    test_loss = sess.run(loss_op, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_mse : {:.3f}\".format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題5　MNISTのモデルを作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\new\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Owner\\anaconda3\\envs\\new\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Owner\\anaconda3\\envs\\new\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\Owner\\anaconda3\\envs\\new\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# 読み込み\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#　平滑化\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "# 正規化\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# 変形\n",
    "y_train = y_train.astype(np.int)[:, np.newaxis]\n",
    "y_test = y_test.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# one-hotベクトル化\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:])\n",
    "y_test_one_hot = enc.fit_transform(y_test[:])\n",
    "\n",
    "# 分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflowで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\new\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_loss : 32.3239, val_loss : 31.4053, train_acc : 0.600, val_acc : 0.606\n",
      "Epoch 1, train_loss : 22.5284, val_loss : 22.3642, train_acc : 0.646, val_acc : 0.643\n",
      "Epoch 2, train_loss : 18.5634, val_loss : 18.2412, train_acc : 0.687, val_acc : 0.685\n",
      "Epoch 3, train_loss : 16.4376, val_loss : 16.3740, train_acc : 0.700, val_acc : 0.697\n",
      "Epoch 4, train_loss : 17.0371, val_loss : 17.0895, train_acc : 0.708, val_acc : 0.708\n",
      "Epoch 5, train_loss : 15.4541, val_loss : 15.3585, train_acc : 0.744, val_acc : 0.744\n",
      "Epoch 6, train_loss : 16.9310, val_loss : 16.9756, train_acc : 0.744, val_acc : 0.738\n",
      "Epoch 7, train_loss : 16.6295, val_loss : 16.8168, train_acc : 0.753, val_acc : 0.753\n",
      "Epoch 8, train_loss : 16.2811, val_loss : 16.0743, train_acc : 0.769, val_acc : 0.763\n",
      "Epoch 9, train_loss : 16.7202, val_loss : 16.4293, train_acc : 0.774, val_acc : 0.778\n",
      "Epoch 10, train_loss : 16.2536, val_loss : 16.6894, train_acc : 0.780, val_acc : 0.776\n",
      "Epoch 11, train_loss : 16.8444, val_loss : 16.5402, train_acc : 0.781, val_acc : 0.780\n",
      "Epoch 12, train_loss : 16.2396, val_loss : 16.3698, train_acc : 0.803, val_acc : 0.802\n",
      "Epoch 13, train_loss : 15.6366, val_loss : 15.8538, train_acc : 0.800, val_acc : 0.795\n",
      "Epoch 14, train_loss : 17.1729, val_loss : 17.1653, train_acc : 0.798, val_acc : 0.797\n",
      "Epoch 15, train_loss : 16.4653, val_loss : 16.6290, train_acc : 0.810, val_acc : 0.809\n",
      "Epoch 16, train_loss : 13.7978, val_loss : 13.7010, train_acc : 0.828, val_acc : 0.822\n",
      "Epoch 17, train_loss : 17.9710, val_loss : 17.6160, train_acc : 0.800, val_acc : 0.800\n",
      "Epoch 18, train_loss : 16.6632, val_loss : 16.3634, train_acc : 0.824, val_acc : 0.828\n",
      "Epoch 19, train_loss : 17.0467, val_loss : 17.3345, train_acc : 0.819, val_acc : 0.815\n",
      "test_acc : 0.817\n"
     ]
    }
   ],
   "source": [
    "# 各種変数定義\n",
    "learning_rate = 0.003\n",
    "batch_size = 1\n",
    "num_epochs = 20\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 10 # 2値分類からの変更箇所\n",
    "\n",
    "# 空配列定義\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# ミニバッチイテレータ生成\n",
    "get_mini_batch_train = GetMiniBatch(X_train[:1000], y_train[:1000], batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"tensorflowを利用したニューラルネットワーク\n",
    "    Parameters\n",
    "    ---------------\n",
    "    x : 入力配列\n",
    "    \"\"\"\n",
    "    # 重み定義\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    # バイアス定義\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    # 計算グラフ構築（順伝播処理）\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
    "    return layer_output\n",
    "\n",
    "# 計算グラフ受け取る\n",
    "logits = example_net(X)\n",
    "# 損失定義\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits)) # 2値分類からの変更箇所\n",
    "# 最適化手法の定義\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "# 最適化手法で、定義した損失を最小化するルールを作成\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# ACC計算\n",
    "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1)) # 2値分類からの変更箇所\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# 変数を扱うためのおまじない\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# tensorflowのセッション開始\n",
    "with tf.Session() as sess:\n",
    "    # 初期化\n",
    "    sess.run(init)\n",
    "    # 学習回数分ループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # ミニバッチイテレータでループ\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # 「最適化手法で、定義した損失を最小化するルール」を実行\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "        # 記録\n",
    "        train_loss, train_acc = sess.run([loss_op, accuracy], feed_dict={X: X_train, Y: y_train})\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        # 仮定出力\n",
    "        print(\"Epoch {}, train_loss : {:.4f}, val_loss : {:.4f}, train_acc : {:.3f}, val_acc : {:.3f}\".format(epoch, train_loss, val_loss, train_acc, val_acc))\n",
    "    \n",
    "    # 学習が終了したらテストデータで実行\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
    "    #prediction = sess.run(logits, feed_dict={X: X_test, Y: y_test})\n",
    "    #print(prediction)\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}