{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 線形回帰とは"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class ScratchLinearRegression:\n",
    "    \n",
    "    \"\"\"\n",
    "    線形回帰のスクラッチ実装\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      訓練データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証データに対する損失の記録\n",
    "    \"\"\"\n",
    "    \n",
    "    #コンストラクタ\n",
    "    def __init__(self,n_iteration,alfa): #★☆★☆初期化するインスタンス変数だけ入れる。ここに入れればメソッドの引数に入れなくてもクラス内メソッドで動作する\n",
    "                \n",
    "        self.n_iteration = n_iteration   # イテレーター最急降下を繰り返す回数\n",
    "        self.alfa = alfa                 # 学習率\n",
    "        self.θ = np.array(None)         # パラメータベクトル(type:ndarray)この時点でシータの要素数は分からないからナンにしておく\n",
    "        #self.loss_func = np.array(None)  # 損失関数　平均二乗誤差を1/2した値　その時のθで計算したy_hatとｙの差分（損失）の値（平均２乗誤差の1/2）\n",
    "        #self.no_bias = no_bias          # バイアス項を入れるかどうか、メソッド呼び出しで定義する\n",
    "        #self.verbose = verbose          # 学習過程を出力するかどうか、メソッド呼び出しで定義する\n",
    "        \n",
    "        # 損失を記録する配列を用意\n",
    "        self.loss = np.array([])         #イテレータ回数分のゼロが入ったndarray　self.n_iterationの数だけ入るのでその要素数分作っておく。★☆★☆np.empty()じゃダメなのか⇒empty()は適当な数字の配列を作るのでnp.zerosとやってることはかわらない\n",
    "        self.val_loss = np.array([])\n",
    "        self.θ_sum = np.array([])\n",
    "        #print(\"self.θ_sum:{}\".format(self.θ_sum)) #------検証中--------\n",
    "        \n",
    "\n",
    "    # 問題6（学習と推定）\n",
    "    def fit(self,X,y, X_val=None, y_val=None):  # コンストラクタ以外のこのメソッドで使う引数のみ入れる。引数はメソッド呼び出し時定義する。それか、メソッドの引数をインスタンス変数に代入して使いたい時に再度インスタンス変数を定義する。\n",
    "        \"\"\"\n",
    "        線形回帰の学習\n",
    "        \"\"\"\n",
    "        \n",
    "        #------Xとyについてのfit(学習)についてのコード--------\n",
    "        #θのndarrayの要素数はXの特徴量の数と同じなので、Xの列数の０が入ったndarrayを作った\n",
    "        self.θ = np.zeros(X.shape[1])# \n",
    "        \n",
    "        #print(\"self.θ:{}\".format(self.θ))        #------検証中--------\n",
    "        #print(type(self.θ))                       #------検証中--------\n",
    "                \n",
    "\n",
    "        #イテレーション回数最急降下法を行う為のfor文\n",
    "        for i in range(self.n_iteration):\n",
    "            \n",
    "            # ------①　問題1（Xデータとself.θ（初期値ゼロ）を代入＆過程関数（y_hat）の算出）《X θ⇒y_hat》------\n",
    "            y_hat = self._linear_hypothesis(X) #★☆★☆これは何を表してるのか謎 #self._linear_hypothesis()の値を受け取る\n",
    "            \n",
    "            # ------② 問題2（θの算出　最急降下法によるパラメータthetaの更新値計算）------\n",
    "            self.θ = self._gradient_descent(X,y,y_hat)\n",
    "            #print(\"self.θ:{}\".format(self.θ))        #------検証中--------\n",
    "            #np.append(self.θ_sum , self.θ, axis=1)\n",
    "            #print(\"self.θ_sum:{}\".format(self.θ_sum))        #------検証中--------\n",
    "            # ------③ 問題3　問題2で新しいself.θが決まった段階で、self.θとXを代入して損失関数を計算する Loss_function（損失関数を出すメソッド）に入れて損失を出す------\n",
    "            \n",
    "            \n",
    "            #------------↓引数Xとその損失関数について------------\n",
    "\n",
    "            #まずXと②で更新したself.θをpredictに代入してy_hatを出力\n",
    "            y_hat = self.predict(X) #★☆★☆ここなんでただのpredictじゃ呼べなくてself.predictだと呼べるの？predict() takes 2 positional arguments but 3 were given\n",
    "\n",
    "            #次に上で出したy_hatを使ってX_trainの損失関数を出力する\n",
    "            loss_func = self.Loss_function(y_hat, y)\n",
    "\n",
    "            #Xの損失関数をndarrayに格納\n",
    "            self.loss = np.append(self.loss , loss_func)\n",
    "            \n",
    "            #print(\"self.loss:{}\".format(self.loss)) #------検証中---------\n",
    "            \n",
    "            y_val_hat = self.predict(X_val) #self._linear_hypothesis()の値を受け取る\n",
    "            \n",
    "            #次に上で出したy_val_hatを使ってX_testの損失関数を出力する\n",
    "            loss_func = self.Loss_function(y_val_hat, y_val)\n",
    "            \n",
    "            #X_valの損失関数を専用のndarrayであるself.val_lossに格納\n",
    "            self.val_loss = np.append(self.val_loss,loss_func)\n",
    "            \n",
    "            #------------↑------------\n",
    "            \n",
    "            \n",
    "        #------X_valとy_valについてのfit(学習)についてのコード--------\n",
    "\n",
    "        #self.θ = np.zeros(X_val.shape[1])# Xのaxis=1の要素数を取得してシータをゼロで初期化した。\n",
    "        #print(\"self.θ:{}\".format(self.θ))        #------検証中--------\n",
    "        #print(type(self.θ))                       #------検証中--------\n",
    "\n",
    "\n",
    "        #イテレーション回数最急降下法を行う為のfor文\n",
    "        #print(\"X_valデータ回すときのi:{}\".format(i)) #------検証中--------\n",
    "        #i = 0\n",
    "        \n",
    "        #         for i in range(self.n_iteration):\n",
    "\n",
    "        #             # ------①　問題1（X_valデータとself.θ（初期値ゼロ）を代入＆過程関数（y_val_hat）の算出）《X θ⇒y_hat》------\n",
    "        #             y_val_hat = self._linear_hypothesis(X_val) #self._linear_hypothesis()の値を受け取る\n",
    "\n",
    "        #             # ------② 問題2（θの算出　最急降下法によるパラメータthetaの更新値計算）------\n",
    "        #             # self.θ = self._gradient_descent(X_val,y_val,y_val_hat)\n",
    "\n",
    "        #             # ------③ 問題3　問題2で新しいself.θが決まった段階で、self.θとXを代入して損失関数を計算する Loss_function（損失関数を出すメソッド）に入れて損失を出す------\n",
    "\n",
    "\n",
    "        #             #------------↓引数X_valとその損失関数について------------\n",
    "\n",
    "        #             #まずX_valと②で更新したself.θをpredictに代入してy_val_hatを出力\n",
    "        #             #y_val_hat = self.predict(X_val) #★☆★☆ここなんでただのpredictじゃ呼べなくてself.predictだと呼べるの？predict() takes 2 positional arguments but 3 were given\n",
    "\n",
    "        #             #次に上で出したy_val_hatを使ってX_testの損失関数を出力する\n",
    "        #             loss_func = self.Loss_function(y_val_hat, y_val)\n",
    "\n",
    "        #             #X_valの損失関数を専用のndarrayであるself.val_lossに格納\n",
    "        #             self.val_loss = np.append(self.val_loss,loss_func)\n",
    "\n",
    "        #             #print(\"type(self.val_loss):{}\".format(type(self.val_loss)))------検証中---------\n",
    "\n",
    "        #             #------------↑------------\n",
    "\n",
    "            \n",
    "            \n",
    "    #問題7　【グラフ描画】トレーニングデータとテストデータの損失関数　横軸にイテレーション回数縦軸に損失関数\n",
    "    def Learnig_curve(self):\n",
    "\n",
    "        #self.lossの要素数をndarrayにした。散布図のX座標で使う為\n",
    "        num = np.arange(len(self.loss))\n",
    "        num_val = np.arange(len(self.val_loss))\n",
    "        \n",
    "        #print(\"num:{}\".format(num))\n",
    "        #print(\"self.loss:{}\".format(self.loss))\n",
    "        #print(\"num_val:{}\".format(num))\n",
    "        #print(\"self.val_loss:{}\".format(self.val_loss))\n",
    "\n",
    "        #散布図の描画\n",
    "        plt.scatter(num, self.loss,color = \"red\" )\n",
    "        plt.scatter(num_val, self.val_loss, color = \"blue\")\n",
    "        plt.colorbar()\n",
    "        plt.grid()\n",
    "        return plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    # 問題1\n",
    "    def _linear_hypothesis(self,X):  # 必要に応じて引数を追加して下さい\n",
    "        \"\"\"\n",
    "        線形の仮定関数を計算する\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          訓練データ\n",
    "          \n",
    "        θ: パラメータベクトル\n",
    "        \n",
    "        Xj : j番目の特徴量\n",
    "        θj: j番目のパラメータ（重み）\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        次の形のndarray, shape (n_samples, 1)\n",
    "        線形の仮定関数による推定結果\n",
    "        \"\"\"\n",
    "        \n",
    "        #1次元配列は転置できないのでreshapeで２次元に変えてから転置する\n",
    "        θ_2dim = self.θ.reshape(( 1, len(self.θ)))\n",
    "        #print(θ_2dim.transpose())\n",
    "        \n",
    "        #n個の特徴量の仮定関数hθにおける特徴量をXjとし一般式としたもの\n",
    "        y_hat = X @ θ_2dim.transpose()  #★☆★☆仮定関数★☆★☆y_hatはすでに計算済みで１次元のndarrayで出てくる。X @ θ_2dim.transpose()の順番にしたのは野田氏メモ参照\n",
    "        \n",
    "        \n",
    "        return y_hat\n",
    "        \n",
    "    # 問題2\n",
    "    #仮定関数を最適化するパラメータθを出力するメソッド\n",
    "    def _gradient_descent(self,X,y,y_hat): #θやalfaはインスタンス変数で定義しているのでここの引数に入れなくていい\n",
    "                        \n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          訓練データ\n",
    "        alfa : 学習率\n",
    "        error : y_hatとｙの差分\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        次の形のndarray, shape (n_samples, 1)\n",
    "        線形の仮定関数による推定結果\n",
    "        説明を記述\n",
    "        \"\"\"\n",
    "        #データの数（行の数）\n",
    "        m = len(X[:,0])\n",
    "        \n",
    "        #特徴量の数（列の数）\n",
    "        n = len(X[0,:])\n",
    "                        \n",
    "        #-------------パラメータθを再急降下法で最適化する一般式-----------\n",
    "\n",
    "        #error(予測値と真の値の差分）のデータ数iの値を１～ｍまで（データ数ある分だけ）合計し、θの一般式self.θ[j] を出して、それを特徴量の数ｎ個分をfor文で作り出す\n",
    "        for j in range(n): # n:特徴量の数（列の数） \n",
    "            \n",
    "            #error(予測値と真の値の差分）にX[i,j]を掛けた値をｍ（データ数の数）個分足すので、足した変数をsum_errorとする\n",
    "            sum_error = 0 #とりあえず定義（初期値）for文の為。値はゼロ\n",
    "            \n",
    "            #ここはerrorをｍ個作ってそれをsum_errorに足していく\n",
    "            for i in range( m ): \n",
    "                \n",
    "                #error(予測値と真の値の差分）\n",
    "                error =y_hat[i] - y[i]\n",
    "                \n",
    "                #sum_errorはデータ数（ｍ）分　error * X[i,j]を繰り返し足し続けた合計値\n",
    "                sum_error += error * X[i,j] #Σi=0~m　((hθ_Xj) - yi)を全部足した\n",
    "                \n",
    "            #最急降下法による次の地点を求める一般式。ここでθを0からｎ個作っていく\n",
    "            self.θ[j] = self.θ[j] - self.alfa * sum_error / m #ここは公式通り。θは２次元ｍ行０列の配列。特徴量の数ｊをｎ個分forして特徴量の数だけself.θ［ｊ］が入ったself.θができた。\n",
    "            \n",
    "        return  self.θ #self.θ[j]を出力してもいいが、下の問題３ではreturnしなくても同じクラス内なのでself.θ[j]は使えるのでNoneとした\n",
    "        \n",
    "    # 問題3\n",
    "    # 更新されたself.θと引数Xを仮定関数に代入して推定する（y_hatを主力する）\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        線形回帰での推定\n",
    "        最適化したθが代入された仮定関数に検証データを入力推定値を算出する\n",
    "        線形回帰を使い推定する。\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_test : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        次の形のndarray, shape (n_samples, 1)\n",
    "        線形回帰による推定結果\n",
    "\n",
    "        \"\"\"\n",
    "        #更新されたself.θを２次元に変えてから転置する\n",
    "        θ_2dim = self.θ.reshape(( 1, len(self.θ)))\n",
    "        #print(θ_2dim.transpose())\n",
    "        \n",
    "        #n個の特徴量の仮定関数hθにおける特徴量をXjとし一般式としたもの\n",
    "        y_hat = X @ θ_2dim.transpose()  #★☆★☆仮定関数★☆★☆y_hatはすでに計算済みで１次元のndarrayで出てくる。X @ θ_2dim.transpose()の順番にしたのは野田氏メモ参照\n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "\n",
    "    # 問題5\n",
    "    #目的関数（損失関数）は平均二乗誤差を1/2した値のこと\n",
    "    def Loss_function(self,y_pred, y):\n",
    "        \"\"\"\n",
    "        損失関数\n",
    "        \"\"\"\n",
    "        #データの数（行の数）\n",
    "        m = len(y_pred)\n",
    "        #         print(\"m:{}\".format(m))\n",
    "        #         print(\"type(m):{}\".format(type(m)))\n",
    "        #         print(\"y_pred:{}\".format(y_pred))\n",
    "        #         print(\"y:{}\".format(y))\n",
    "        \n",
    "        #平均二乗誤差mseを定義\n",
    "        mse = 0\n",
    "        sum_error = 0\n",
    "        #         print(\"y_predのデータです:{}\".format(y_pred))\n",
    "        #         print(\"y_predのデータサイズです:{}\".format(y_pred.size))\n",
    "        #         print(\"yのデータです:{}\".format(y))\n",
    "        #         print(\"yのデータサイズです:{}\".format(y.size))\n",
    "        \n",
    "\n",
    "        #平均二乗誤差を計算\n",
    "        for i in range(m):\n",
    "            error = (y_pred[i] - y[i] )**2\n",
    "            sum_error += error\n",
    "            # print(\"i:{} ,y_pred[i]:{} , y[i]:{}\".format(i,y_pred[i],y[i]))# ------検証中--------\n",
    "            # print(\"sum_error:{} \".format(sum_error))# ------検証中--------\n",
    "            # print(\"\\n\")# ------検証中--------\n",
    "        \n",
    "        #平均二乗誤差を1/2して損失関数を出す\n",
    "        self.loss_func = sum_error / 2 * m\n",
    "        \n",
    "        #損失関数を出力する\n",
    "        return self.loss_func\n",
    "\n",
    "#------------↑クラスはここまで-----------\n",
    "\n",
    "\n",
    "\n",
    "# 問題4(ここは使ってない)\n",
    "# ２つの目的変数（y_predとy）の平均二乗誤差を求める関数\n",
    "def MSE(y_pred, y):\n",
    "    \"\"\"\n",
    "    平均二乗誤差の計算\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : 次の形のndarray, shape (n_samples,)\n",
    "      推定した値\n",
    "    y : 次の形のndarray, shape (n_samples,)\n",
    "      正解値\n",
    "    Returns\n",
    "    ----------\n",
    "    mse : numpy.float\n",
    "      平均二乗誤差\n",
    "    \"\"\"\n",
    "    \n",
    "    #データの数（行の数）\n",
    "    m = len(y_pred)\n",
    "    # print(\"m:{}\".format(m))\n",
    "    \n",
    "    #平均二乗誤差mseを定義\n",
    "    mse = 0\n",
    "    sum_error = 0\n",
    "    \n",
    "    #平均二乗誤差を計算\n",
    "    for i in range(m):\n",
    "        error = (y_pred[i] - y[i] )**2\n",
    "        sum_error += error\n",
    "        print(\"i:{} ,y_pred[i]:{} , y[i]:{}\".format(i,y_pred[i],y[i]))\n",
    "        print(\"sum_error:{} \".format(sum_error))\n",
    "    mse = sum_error / m\n",
    "    print(\"sum:{} \".format(mse))    \n",
    "    \n",
    "    return mse\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題6】学習と推定\n",
    "機械学習スクラッチ入門のSprintで用意したHouse Pricesコンペティションのデータに対してスクラッチ実装の学習と推定を行なう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1710</td>\n",
       "      <td>2003</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1262</td>\n",
       "      <td>1976</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1786</td>\n",
       "      <td>2001</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1717</td>\n",
       "      <td>1915</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2198</td>\n",
       "      <td>2000</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GrLivArea  YearBuilt  SalePrice\n",
       "0       1710       2003     208500\n",
       "1       1262       1976     181500\n",
       "2       1786       2001     223500\n",
       "3       1717       1915     140000\n",
       "4       2198       2000     250000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-------------House Pricesデータセットを学習・推定-----------\n",
    "\n",
    "#　library\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#　データフレームのインポートと前処理\n",
    "df = pd.read_csv(\"train.csv\") \n",
    "df_selected = df.loc[:, [\"GrLivArea\", \"YearBuilt\", \"SalePrice\"]]\n",
    "df_selected.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "・GrLivAreaとYearBuiltを標準化したデータフレーム \n",
      "\n",
      "      GrLivArea  YearBuilt  SalePrice\n",
      "0      0.370333   1.050994     208500\n",
      "1     -0.482512   0.156734     181500\n",
      "2      0.515013   0.984752     223500\n",
      "3      0.383659  -1.863632     140000\n",
      "4      1.299326   0.951632     250000\n",
      "...         ...        ...        ...\n",
      "1455   0.250402   0.918511     175000\n",
      "1456   1.061367   0.222975     210000\n",
      "1457   1.569647  -1.002492     266500\n",
      "1458  -0.832788  -0.704406     142125\n",
      "1459  -0.493934  -0.207594     147500\n",
      "\n",
      "[1460 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#-------------GrLivArea　・　YearBuilt　だけを標準化する。-----------\n",
    "\n",
    "#データフレームのコピーを作成\n",
    "scaled_features = df_selected.copy()\n",
    "\n",
    "#標準化する２つのカラムを定義する\n",
    "col_names = ['GrLivArea', 'YearBuilt']\n",
    "\n",
    "#featuresとして、標準化するカラムのみのデータフレームを作る\n",
    "features = scaled_features[col_names]\n",
    "\n",
    "#scalerという変数でStandardScalerクラスをインスタンス化\n",
    "scaler = StandardScaler().fit(features.values) #データフレームをndarrayに変換する.valuesメソッド\n",
    "\n",
    "#featuresという変数でscalerクラスのtransformメソッド（標準化）を呼び出し\n",
    "features = scaler.transform(features.values) #データフレームをndarrayに変換する.valuesメソッド\n",
    "\n",
    "#上のscaled_features[col_names]に標準化したデータfeatures を上書き\n",
    "scaled_features[col_names] = features\n",
    "\n",
    "#出力\n",
    "print(\"・GrLivAreaとYearBuiltを標準化したデータフレーム \\n\")\n",
    "\n",
    "#scaled_featuresのGrLivAreaとYearBuiltは標準化された上書きデータが代入された為、使用DFはscaled_features\n",
    "print(scaled_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xのバイアス項追加:\n",
      "      bias  GrLivArea  YearBuilt\n",
      "0      1.0   0.370333   1.050994\n",
      "1      1.0  -0.482512   0.156734\n",
      "2      1.0   0.515013   0.984752\n",
      "3      1.0   0.383659  -1.863632\n",
      "4      1.0   1.299326   0.951632\n",
      "...    ...        ...        ...\n",
      "1455   1.0   0.250402   0.918511\n",
      "1456   1.0   1.061367   0.222975\n",
      "1457   1.0   1.569647  -1.002492\n",
      "1458   1.0  -0.832788  -0.704406\n",
      "1459   1.0  -0.493934  -0.207594\n",
      "\n",
      "[1460 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#-------------訓練データと検証データに分割-----------\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#train_test_splitに代入してデータを分ける為、特徴量のDFと目的変数のDFに分割する\n",
    "X = scaled_features.loc[: , ['GrLivArea','YearBuilt']] #[]の中に[]をつけているのでタイプはDFになる。\n",
    "y = scaled_features.loc[: , ['SalePrice']]\n",
    "\n",
    "# #データフレームの小数点を表示しない\n",
    "# pd.options.display.float_format = '{:.0f}'.format\n",
    "\n",
    "#特徴量X［j=0］（切片orバイアス項と呼ぶ）を作るために［j=0］＝1の行をデータの数分のndarrayを作りDFに変換する\n",
    "#バイアス項の名前を指定\n",
    "column = [\"bias\"]\n",
    "df_bias =pd.DataFrame(np.ones(len(X)),columns=column)\n",
    "#print(\"df_bias:{}\".format(df_bias))\n",
    "\n",
    "#concatでバイアス項とX訓練用データを結合しバイアス項が入った訓練用データができた\n",
    "X = pd.concat([df_bias,X], axis=1)\n",
    "\n",
    "print(\"Xのバイアス項追加:\\n{}\".format(X))\n",
    "\n",
    "#自作scratch_train_test_splitは入力X、yがndarrayを想定している為、DFをndarrayに変換\n",
    "X = X.values #.valuesメソッドでndarrayに変換\n",
    "y = y.values #.valuesメソッドでndarrayに変換\n",
    "\n",
    "#train_test_split訓練データと検証データに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=True\n",
    ")\n",
    "\n",
    "#print(\"X_train:{}\".format(X_train))\n",
    "#print(\" X_test:{}\".format( X_test))\n",
    "#print(\"y_train:{}\".format(y_train))\n",
    "#print(\"y_test:{}\".format(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#-------------【訓練用トレーニングデータの代入】スクラッチ実装の学習-----------\n",
    "\n",
    "# debug code試験的にデータ数を制限する\n",
    "# X_train = X_train[:15]\n",
    "# y_train = y_train[:50]\n",
    "# X_test = X_test[:50]\n",
    "# y_test = y_test[:50]\n",
    "\n",
    "#House Pricesという名前でコンペティションクラスを定義\n",
    "house_price = ScratchLinearRegression(n_iteration=100,alfa = 0.05)\n",
    "\n",
    "#house_priceクラスのfitメソッドで訓練用データを学習。パラメータθを算出\n",
    "house_price.fit(X_train,y_train,X_test, y_test)\n",
    "\n",
    "#学習曲線を描く関数を呼び出し\n",
    "house_price.Learnig_curve()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat_test:[[263360.57725362]\n",
      " [154641.69733194]\n",
      " [127061.59109737]\n",
      " [235549.50958828]\n",
      " [137861.49404834]\n",
      " [ 67887.76305043]\n",
      " [211686.77900118]\n",
      " [130535.9909381 ]\n",
      " [496595.99065891]\n",
      " [172974.72001262]\n",
      " [199635.28577301]\n",
      " [200873.2763628 ]\n",
      " [254466.16837659]\n",
      " [114837.84043699]\n",
      " [113090.55342242]\n",
      " [145220.08709379]\n",
      " [241324.46255349]\n",
      " [151615.16997992]\n",
      " [146068.45850801]\n",
      " [152147.1272754 ]\n",
      " [147033.10329334]\n",
      " [147989.13758081]\n",
      " [102437.9777809 ]\n",
      " [199914.07145243]\n",
      " [218085.03251023]\n",
      " [109754.63713808]\n",
      " [215169.33865436]\n",
      " [ 91381.04865036]\n",
      " [234704.99273757]\n",
      " [125237.69514789]\n",
      " [191700.53249177]\n",
      " [227800.16274124]\n",
      " [127394.77552643]\n",
      " [276649.61038617]\n",
      " [269971.43657168]\n",
      " [198539.63330312]\n",
      " [219160.06010629]\n",
      " [118238.35128029]\n",
      " [269521.9787715 ]\n",
      " [312152.45552602]\n",
      " [221003.44630357]\n",
      " [147676.57802558]\n",
      " [194469.5829788 ]\n",
      " [245177.15113445]\n",
      " [330106.05484044]\n",
      " [193184.21882578]\n",
      " [ 95049.46542315]\n",
      " [123555.68663288]\n",
      " [203236.83873479]\n",
      " [ 96070.09426958]\n",
      " [325985.46032999]\n",
      " [129787.12258468]\n",
      " [167872.02646591]\n",
      " [ 76930.63386293]\n",
      " [210776.98365091]\n",
      " [116220.43839875]\n",
      " [108218.82130649]\n",
      " [242768.4844513 ]\n",
      " [132440.35038602]\n",
      " [ 84268.60203455]\n",
      " [125849.66525637]\n",
      " [118019.85491089]\n",
      " [137547.34918166]\n",
      " [136524.00039775]\n",
      " [219391.02153707]\n",
      " [161352.51039621]\n",
      " [106215.64273839]\n",
      " [236183.23919662]\n",
      " [ 90315.76617819]\n",
      " [234010.97381921]\n",
      " [205436.30348701]\n",
      " [103290.65444405]\n",
      " [103652.62355938]\n",
      " [207260.19943649]\n",
      " [109363.43265106]\n",
      " [228555.60559565]\n",
      " [119405.17281013]\n",
      " [ 80014.27990207]\n",
      " [307790.01983486]\n",
      " [151279.26561338]\n",
      " [131824.0750286 ]\n",
      " [122673.09065743]\n",
      " [ 99857.50435097]\n",
      " [151615.16997992]\n",
      " [308152.43963561]\n",
      " [195057.07364994]\n",
      " [ 91945.64519561]\n",
      " [200174.95219551]\n",
      " [188966.39050659]\n",
      " [123236.10189123]\n",
      " [223823.04097672]\n",
      " [190446.22227709]\n",
      " [202620.56337737]\n",
      " [245567.90493605]\n",
      " [190388.65290455]\n",
      " [142322.29817427]\n",
      " [210551.4620951 ]\n",
      " [140297.81079173]\n",
      " [114399.26238674]\n",
      " [142773.34128589]\n",
      " [236177.79932165]\n",
      " [252602.60799091]\n",
      " [147777.66639783]\n",
      " [176921.92246483]\n",
      " [ 56796.60935865]\n",
      " [288884.24079648]\n",
      " [187173.99918086]\n",
      " [ 87793.54606141]\n",
      " [211585.69062893]\n",
      " [104781.36596447]\n",
      " [ 38725.60204708]\n",
      " [121321.99731942]\n",
      " [226491.45377691]\n",
      " [107323.76026966]\n",
      " [202232.52951325]\n",
      " [187888.64297305]\n",
      " [298835.77233323]\n",
      " [124472.50716957]\n",
      " [238045.66495627]\n",
      " [259344.92567894]\n",
      " [170810.61444766]\n",
      " [204604.25169768]\n",
      " [117288.89149382]\n",
      " [221913.24165384]\n",
      " [215610.63664209]\n",
      " [215132.39415565]\n",
      " [275280.61211182]\n",
      " [194618.49559969]\n",
      " [188716.38951344]\n",
      " [185429.43210377]\n",
      " [189540.28149033]\n",
      " [188618.02107867]\n",
      " [259062.28543601]\n",
      " [182435.99468697]\n",
      " [114399.26238674]\n",
      " [223626.30410718]\n",
      " [144852.67810349]\n",
      " [193134.80926569]\n",
      " [101491.2379319 ]\n",
      " [209846.56342681]\n",
      " [122044.80092405]\n",
      " [171057.89550333]\n",
      " [234041.34381694]\n",
      " [138951.70664327]\n",
      " [152122.19715264]\n",
      " [200847.21161401]\n",
      " [211883.51587073]\n",
      " [121614.38268625]\n",
      " [237948.43114753]\n",
      " [205329.77523979]\n",
      " [163378.5830902 ]\n",
      " [236132.69501049]\n",
      " [244738.5730842 ]\n",
      " [173423.49387218]\n",
      " [146750.46305041]\n",
      " [311540.48541753]\n",
      " [106534.09285401]\n",
      " [136230.48040489]\n",
      " [177848.03744   ]\n",
      " [197943.98281953]\n",
      " [115433.49092058]\n",
      " [134894.12138032]\n",
      " [221669.81516169]\n",
      " [123842.63212475]\n",
      " [221473.07829215]\n",
      " [125888.19506653]\n",
      " [166180.72351243]\n",
      " [286874.48772738]\n",
      " [198157.03931396]\n",
      " [226678.44552256]\n",
      " [157080.28332738]\n",
      " [193599.45206472]\n",
      " [194076.10923971]\n",
      " [158835.7301544 ]\n",
      " [110049.29175697]\n",
      " [139533.75743945]\n",
      " [130616.45443653]\n",
      " [290680.93737114]\n",
      " [110697.07173814]\n",
      " [149091.81523712]\n",
      " [252575.40861609]\n",
      " [216207.87243713]\n",
      " [ 87673.41812679]\n",
      " [235295.2033462 ]\n",
      " [ 83077.30106737]\n",
      " [198796.65948269]\n",
      " [126498.57986356]\n",
      " [186650.65238325]\n",
      " [204553.70751155]\n",
      " [147989.13758081]\n",
      " [151615.16997992]\n",
      " [151518.3868566 ]\n",
      " [215568.25226841]\n",
      " [146207.62600501]\n",
      " [ 88444.04598006]\n",
      " [142551.67429359]\n",
      " [ 91381.04865036]\n",
      " [128941.47110794]\n",
      " [189935.34054086]\n",
      " [164716.0767408 ]\n",
      " [148713.52649689]\n",
      " [168821.48625238]\n",
      " [ 74912.72098139]\n",
      " [139985.9351771 ]\n",
      " [163214.93615586]\n",
      " [287477.16339738]\n",
      " [146262.47544007]\n",
      " [272223.03082148]\n",
      " [237254.41222917]\n",
      " [ 56482.46449197]\n",
      " [174155.14122987]\n",
      " [222940.44500126]\n",
      " [111027.53622972]\n",
      " [ 58955.27504865]\n",
      " [221980.10546486]\n",
      " [258354.66683024]\n",
      " [175446.39594327]\n",
      " [314502.41821059]\n",
      " [229539.28994336]\n",
      " [ 93449.50571803]\n",
      " [184549.5560658 ]\n",
      " [164269.3388781 ]\n",
      " [128187.61356498]\n",
      " [116467.71945442]\n",
      " [213749.79619389]\n",
      " [264800.2939025 ]\n",
      " [250877.0804762 ]\n",
      " [210991.62545679]\n",
      " [141684.263317  ]\n",
      " [170379.06158383]\n",
      " [109993.30769588]\n",
      " [105893.33805926]\n",
      " [115433.49092058]\n",
      " [163371.55790379]\n",
      " [236212.02388289]\n",
      " [136244.0800923 ]\n",
      " [209822.08398947]\n",
      " [ 94802.18436748]\n",
      " [127215.94359323]\n",
      " [164455.19599772]\n",
      " [131926.74871231]\n",
      " [224168.69046716]\n",
      " [237206.58798053]\n",
      " [237156.0437944 ]\n",
      " [159755.27062857]\n",
      " [152981.44831679]\n",
      " [177669.2055068 ]\n",
      " [220191.56870264]\n",
      " [127047.99140996]\n",
      " [200166.79238306]\n",
      " [159635.14269394]\n",
      " [257377.55698352]\n",
      " [ 64756.29276265]\n",
      " [356336.20236304]\n",
      " [198999.97085322]\n",
      " [245177.15113445]\n",
      " [ 88740.2859104 ]\n",
      " [222015.46465213]\n",
      " [386088.57397501]\n",
      " [623704.3280078 ]\n",
      " [230464.2702925 ]\n",
      " [146918.41523368]\n",
      " [189432.61861708]\n",
      " [ 84301.69196976]\n",
      " [272675.20855914]\n",
      " [243462.50336966]\n",
      " [297465.63943285]\n",
      " [ 36088.69387064]\n",
      " [196589.03491802]\n",
      " [235345.74753232]\n",
      " [127666.98670486]\n",
      " [188509.22357939]\n",
      " [146426.57305983]\n",
      " [194729.32909584]\n",
      " [188686.92088656]\n",
      " [269572.52295763]\n",
      " [220366.54607233]\n",
      " [169543.15523098]\n",
      " [256194.41582879]\n",
      " [109117.73690684]\n",
      " [119418.77249754]\n",
      " [259342.20574146]\n",
      " [147412.97734502]\n",
      " [156811.24277186]\n",
      " [193929.9165563 ]\n",
      " [219873.56927245]\n",
      " [151290.14536331]\n",
      " [222291.53039407]\n",
      " [304990.59935011]\n",
      " [222039.26014886]\n",
      " [ 68026.93054743]\n",
      " [ 86984.83908339]]\n"
     ]
    }
   ],
   "source": [
    "#-------------スクラッチ実装の推定　＆　scikit-learnによる実装との比較-----------\n",
    "\n",
    "y_hat_test = house_price.predict(X_test)\n",
    "print(\"y_hat_test:{}\".format(y_hat_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(292, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "ｙの予測したデータ\n",
      "[[264908]\n",
      " [155745]\n",
      " [127984]\n",
      " [236857]\n",
      " [138726]\n",
      " [ 68733]\n",
      " [212720]\n",
      " [131544]\n",
      " [499072]\n",
      " [173836]\n",
      " [200616]\n",
      " [202018]\n",
      " [255750]\n",
      " [115611]\n",
      " [113956]\n",
      " [146194]\n",
      " [242581]\n",
      " [152425]\n",
      " [146999]\n",
      " [153376]\n",
      " [148042]\n",
      " [148729]\n",
      " [103172]\n",
      " [200959]\n",
      " [219323]\n",
      " [110951]\n",
      " [216257]\n",
      " [ 92025]\n",
      " [235851]\n",
      " [126167]\n",
      " [192895]\n",
      " [228986]\n",
      " [128172]\n",
      " [278036]\n",
      " [271283]\n",
      " [199567]\n",
      " [220241]\n",
      " [119002]\n",
      " [270858]\n",
      " [313743]\n",
      " [222382]\n",
      " [148673]\n",
      " [195430]\n",
      " [246389]\n",
      " [331778]\n",
      " [194354]\n",
      " [ 95790]\n",
      " [124324]\n",
      " [204382]\n",
      " [ 96863]\n",
      " [327596]\n",
      " [130644]\n",
      " [168851]\n",
      " [ 77657]\n",
      " [211901]\n",
      " [116980]\n",
      " [109260]\n",
      " [243964]\n",
      " [133321]\n",
      " [ 84994]\n",
      " [126698]\n",
      " [118867]\n",
      " [138484]\n",
      " [137419]\n",
      " [220531]\n",
      " [162405]\n",
      " [107005]\n",
      " [237325]\n",
      " [ 91270]\n",
      " [235174]\n",
      " [206455]\n",
      " [104155]\n",
      " [104451]\n",
      " [208272]\n",
      " [110168]\n",
      " [229677]\n",
      " [120227]\n",
      " [ 80813]\n",
      " [309301]\n",
      " [152246]\n",
      " [132612]\n",
      " [123427]\n",
      " [100858]\n",
      " [152425]\n",
      " [309976]\n",
      " [196031]\n",
      " [ 92882]\n",
      " [201163]\n",
      " [189879]\n",
      " [124098]\n",
      " [224963]\n",
      " [191540]\n",
      " [203673]\n",
      " [246793]\n",
      " [191324]\n",
      " [143266]\n",
      " [211595]\n",
      " [141453]\n",
      " [115155]\n",
      " [143877]\n",
      " [237341]\n",
      " [253856]\n",
      " [148764]\n",
      " [177945]\n",
      " [ 57493]\n",
      " [290377]\n",
      " [188162]\n",
      " [ 88599]\n",
      " [212629]\n",
      " [105785]\n",
      " [ 39414]\n",
      " [122159]\n",
      " [227786]\n",
      " [108208]\n",
      " [203262]\n",
      " [188970]\n",
      " [300314]\n",
      " [125314]\n",
      " [239413]\n",
      " [260615]\n",
      " [171661]\n",
      " [205604]\n",
      " [118106]\n",
      " [223202]\n",
      " [216705]\n",
      " [216172]\n",
      " [276629]\n",
      " [195575]\n",
      " [189644]\n",
      " [186499]\n",
      " [190519]\n",
      " [189545]\n",
      " [260473]\n",
      " [183464]\n",
      " [115155]\n",
      " [224765]\n",
      " [145914]\n",
      " [194115]\n",
      " [102268]\n",
      " [210950]\n",
      " [122943]\n",
      " [171905]\n",
      " [235468]\n",
      " [139790]\n",
      " [153066]\n",
      " [201902]\n",
      " [212918]\n",
      " [122463]\n",
      " [239120]\n",
      " [206380]\n",
      " [164404]\n",
      " [237280]\n",
      " [245933]\n",
      " [174834]\n",
      " [147900]\n",
      " [313212]\n",
      " [107425]\n",
      " [137308]\n",
      " [178717]\n",
      " [198990]\n",
      " [116189]\n",
      " [135807]\n",
      " [222757]\n",
      " [124644]\n",
      " [222560]\n",
      " [126968]\n",
      " [167225]\n",
      " [288331]\n",
      " [199141]\n",
      " [227822]\n",
      " [158085]\n",
      " [194687]\n",
      " [195035]\n",
      " [159718]\n",
      " [110868]\n",
      " [140406]\n",
      " [131504]\n",
      " [292272]\n",
      " [111677]\n",
      " [149948]\n",
      " [253934]\n",
      " [217469]\n",
      " [ 88562]\n",
      " [236444]\n",
      " [ 83839]\n",
      " [199973]\n",
      " [127313]\n",
      " [187568]\n",
      " [205558]\n",
      " [148729]\n",
      " [152425]\n",
      " [152512]\n",
      " [216637]\n",
      " [146981]\n",
      " [ 89400]\n",
      " [143370]\n",
      " [ 92025]\n",
      " [129831]\n",
      " [191100]\n",
      " [165711]\n",
      " [149699]\n",
      " [169747]\n",
      " [ 75634]\n",
      " [140824]\n",
      " [164492]\n",
      " [289079]\n",
      " [147205]\n",
      " [273588]\n",
      " [238444]\n",
      " [ 57251]\n",
      " [175022]\n",
      " [224066]\n",
      " [111872]\n",
      " [ 59684]\n",
      " [223201]\n",
      " [259836]\n",
      " [176462]\n",
      " [316146]\n",
      " [230665]\n",
      " [ 94093]\n",
      " [185593]\n",
      " [165278]\n",
      " [129326]\n",
      " [117223]\n",
      " [214804]\n",
      " [266113]\n",
      " [252138]\n",
      " [212238]\n",
      " [142620]\n",
      " [171375]\n",
      " [110838]\n",
      " [106787]\n",
      " [116189]\n",
      " [164234]\n",
      " [237433]\n",
      " [137269]\n",
      " [211020]\n",
      " [ 95547]\n",
      " [128113]\n",
      " [165507]\n",
      " [132889]\n",
      " [225305]\n",
      " [238391]\n",
      " [238345]\n",
      " [160700]\n",
      " [153840]\n",
      " [178659]\n",
      " [221282]\n",
      " [128023]\n",
      " [201187]\n",
      " [160663]\n",
      " [258638]\n",
      " [ 65523]\n",
      " [358078]\n",
      " [199961]\n",
      " [246389]\n",
      " [ 89503]\n",
      " [223099]\n",
      " [387991]\n",
      " [626735]\n",
      " [231631]\n",
      " [147990]\n",
      " [190637]\n",
      " [ 85279]\n",
      " [274005]\n",
      " [244640]\n",
      " [299100]\n",
      " [ 36690]\n",
      " [197923]\n",
      " [236489]\n",
      " [128724]\n",
      " [189856]\n",
      " [147496]\n",
      " [195828]\n",
      " [190109]\n",
      " [270903]\n",
      " [221542]\n",
      " [170725]\n",
      " [257460]\n",
      " [110111]\n",
      " [120188]\n",
      " [260623]\n",
      " [148476]\n",
      " [157905]\n",
      " [194883]\n",
      " [221242]\n",
      " [152215]\n",
      " [223450]\n",
      " [306472]\n",
      " [223602]\n",
      " [ 68715]\n",
      " [ 87870]]\n",
      "\n",
      "\n",
      "(292, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "\n",
      "\n",
      "ｙの実際のデータ\n",
      "[[200624]\n",
      " [133000]\n",
      " [110000]\n",
      " [192000]\n",
      " [ 88000]\n",
      " [ 85000]\n",
      " [282922]\n",
      " [141000]\n",
      " [745000]\n",
      " [148800]\n",
      " [208900]\n",
      " [136905]\n",
      " [225000]\n",
      " [123000]\n",
      " [119200]\n",
      " [145000]\n",
      " [190000]\n",
      " [123600]\n",
      " [149350]\n",
      " [155000]\n",
      " [166000]\n",
      " [144500]\n",
      " [110000]\n",
      " [174000]\n",
      " [185000]\n",
      " [168000]\n",
      " [177500]\n",
      " [ 84500]\n",
      " [320000]\n",
      " [118500]\n",
      " [110000]\n",
      " [213000]\n",
      " [156000]\n",
      " [250000]\n",
      " [372500]\n",
      " [175000]\n",
      " [277500]\n",
      " [112500]\n",
      " [263000]\n",
      " [325000]\n",
      " [243000]\n",
      " [130000]\n",
      " [164990]\n",
      " [280000]\n",
      " [403000]\n",
      " [119000]\n",
      " [125000]\n",
      " [128200]\n",
      " [172500]\n",
      " [ 84900]\n",
      " [412500]\n",
      " [156000]\n",
      " [167900]\n",
      " [100000]\n",
      " [275000]\n",
      " [123000]\n",
      " [132000]\n",
      " [239900]\n",
      " [139000]\n",
      " [115000]\n",
      " [137500]\n",
      " [135000]\n",
      " [134450]\n",
      " [180500]\n",
      " [193500]\n",
      " [156500]\n",
      " [132000]\n",
      " [224500]\n",
      " [139000]\n",
      " [225000]\n",
      " [188500]\n",
      " [118000]\n",
      " [ 82000]\n",
      " [392000]\n",
      " [112000]\n",
      " [248900]\n",
      " [134500]\n",
      " [ 79500]\n",
      " [320000]\n",
      " [158000]\n",
      " [140000]\n",
      " [136500]\n",
      " [107500]\n",
      " [145000]\n",
      " [200500]\n",
      " [185000]\n",
      " [105000]\n",
      " [202665]\n",
      " [186000]\n",
      " [136000]\n",
      " [200500]\n",
      " [190000]\n",
      " [187500]\n",
      " [200000]\n",
      " [172500]\n",
      " [157000]\n",
      " [213000]\n",
      " [185000]\n",
      " [124500]\n",
      " [162900]\n",
      " [260000]\n",
      " [198500]\n",
      " [120000]\n",
      " [159500]\n",
      " [105900]\n",
      " [260000]\n",
      " [143000]\n",
      " [106500]\n",
      " [178900]\n",
      " [127000]\n",
      " [ 90350]\n",
      " [118500]\n",
      " [190000]\n",
      " [119900]\n",
      " [183900]\n",
      " [155000]\n",
      " [386250]\n",
      " [133000]\n",
      " [193500]\n",
      " [270000]\n",
      " [141000]\n",
      " [146000]\n",
      " [128500]\n",
      " [176000]\n",
      " [214000]\n",
      " [222000]\n",
      " [415298]\n",
      " [187750]\n",
      " [199900]\n",
      " [180000]\n",
      " [206300]\n",
      " [194000]\n",
      " [142953]\n",
      " [182900]\n",
      " [116050]\n",
      " [213250]\n",
      " [139500]\n",
      " [179000]\n",
      " [107900]\n",
      " [175900]\n",
      " [158500]\n",
      " [145000]\n",
      " [217000]\n",
      " [150500]\n",
      " [108959]\n",
      " [165600]\n",
      " [201000]\n",
      " [145500]\n",
      " [319900]\n",
      " [215000]\n",
      " [180500]\n",
      " [367294]\n",
      " [239000]\n",
      " [145900]\n",
      " [161000]\n",
      " [250000]\n",
      " [ 89471]\n",
      " [230000]\n",
      " [147000]\n",
      " [163900]\n",
      " [ 97000]\n",
      " [142000]\n",
      " [197000]\n",
      " [129000]\n",
      " [232000]\n",
      " [115000]\n",
      " [175000]\n",
      " [265000]\n",
      " [207000]\n",
      " [181000]\n",
      " [176000]\n",
      " [171000]\n",
      " [196000]\n",
      " [176000]\n",
      " [113000]\n",
      " [139000]\n",
      " [135000]\n",
      " [240000]\n",
      " [112000]\n",
      " [134000]\n",
      " [316600]\n",
      " [170000]\n",
      " [116000]\n",
      " [306000]\n",
      " [ 82500]\n",
      " [175000]\n",
      " [106000]\n",
      " [194000]\n",
      " [194201]\n",
      " [155900]\n",
      " [138000]\n",
      " [177000]\n",
      " [214000]\n",
      " [148000]\n",
      " [127000]\n",
      " [142500]\n",
      " [ 80000]\n",
      " [145000]\n",
      " [171000]\n",
      " [122500]\n",
      " [139000]\n",
      " [189000]\n",
      " [120500]\n",
      " [124000]\n",
      " [160000]\n",
      " [200000]\n",
      " [160000]\n",
      " [313000]\n",
      " [275000]\n",
      " [ 67000]\n",
      " [159000]\n",
      " [251000]\n",
      " [ 92900]\n",
      " [109500]\n",
      " [385000]\n",
      " [129000]\n",
      " [ 82500]\n",
      " [301000]\n",
      " [249700]\n",
      " [ 81000]\n",
      " [187500]\n",
      " [110000]\n",
      " [117000]\n",
      " [128500]\n",
      " [213490]\n",
      " [284000]\n",
      " [230500]\n",
      " [190000]\n",
      " [135000]\n",
      " [152000]\n",
      " [ 87500]\n",
      " [155000]\n",
      " [115000]\n",
      " [144000]\n",
      " [248000]\n",
      " [132500]\n",
      " [136000]\n",
      " [117000]\n",
      " [ 82000]\n",
      " [157500]\n",
      " [110000]\n",
      " [181000]\n",
      " [192500]\n",
      " [223500]\n",
      " [181500]\n",
      " [170000]\n",
      " [187500]\n",
      " [185900]\n",
      " [160000]\n",
      " [192000]\n",
      " [181900]\n",
      " [266000]\n",
      " [ 99900]\n",
      " [438780]\n",
      " [229456]\n",
      " [216837]\n",
      " [110500]\n",
      " [175900]\n",
      " [538000]\n",
      " [160000]\n",
      " [172500]\n",
      " [108000]\n",
      " [131500]\n",
      " [106250]\n",
      " [385000]\n",
      " [370878]\n",
      " [345000]\n",
      " [ 68500]\n",
      " [250000]\n",
      " [245350]\n",
      " [125000]\n",
      " [234000]\n",
      " [145000]\n",
      " [181000]\n",
      " [104000]\n",
      " [233000]\n",
      " [164000]\n",
      " [219500]\n",
      " [195000]\n",
      " [108000]\n",
      " [149900]\n",
      " [315000]\n",
      " [177500]\n",
      " [140000]\n",
      " [193879]\n",
      " [137900]\n",
      " [118000]\n",
      " [324000]\n",
      " [555000]\n",
      " [136000]\n",
      " [ 82500]\n",
      " [101000]]\n"
     ]
    }
   ],
   "source": [
    "#-------------既存の線形回帰》による学習と予測-----------\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#訓練用データのデータを用意\n",
    "X = X_train\n",
    "y = y_train\n",
    "\n",
    "# 予測モデルを作成\n",
    "clf = LinearRegression().fit(X, y)\n",
    "\n",
    "# 決定係数\n",
    "clf.score(X, y)\n",
    "\n",
    "# 回帰係数\n",
    "clf.coef_\n",
    "\n",
    "# 切片 (誤差)\n",
    "clf.intercept_\n",
    "\n",
    "#予測モデルに検証用のXを代入してｙの予測データを作成\n",
    "y_test_pred2 = clf.predict(X_test)\n",
    "\n",
    "#小数点以下の値を切り捨て\n",
    "y_test_pred1 = np.floor(y_test_pred2)\n",
    "\n",
    "#小数点切り捨てはfloatなのでintに変換\n",
    "y_test_pred = y_test_pred1.astype(int)\n",
    "\n",
    "print(y_test_pred.shape)\n",
    "print(type(y_test_pred))  #★☆★☆予測したデータはndarrayで出てきた。DFとndarrayで検証とかしていいの？\n",
    "\n",
    "#ｙの予測したデータを出力\n",
    "print(\"ｙの予測したデータ\\n{}\".format(y_test_pred))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "#ｙの実際のデータをndarrayに変換（予測したデータがndarrayなのと、グラフはndarrayを使う為）\n",
    "y_test_ndarray = np.array(y_test)\n",
    "print(y_test_ndarray.shape)\n",
    "print(type(y_test_ndarray))\n",
    "print(\"\\n\")\n",
    "\n",
    "#ｙの実際のデータを出力\n",
    "print(\"ｙの実際のデータ\\n{}\".format(y_test_ndarray))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#トイデータの遊び\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# x = np.linspace(1,6,5)\n",
    "# X = np.c_[np.ones(5),x]#入力データX\n",
    "\n",
    "# y = 2*x + 1#適当な真のモデル\n",
    "# # print(\"Xの行数:\\n{}\".format(X))\n",
    "# # print(\"yの行数:\\n{}\".format(y))\n",
    "\n",
    "# #jyutakuクラスを定義\n",
    "# jyutaku = ScratchLinearRegression(n_iteration=100,alfa = 0.1)\n",
    "\n",
    "# jyutaku.fit(X,y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#トイデータの遊び\n",
    "\n",
    "# import numpy as np\n",
    "# y_pred = np.array([0,1,2,3,4,5])\n",
    "# y = np.array([1,3,5,7,9,11])\n",
    "\n",
    "\n",
    "\n",
    "# # #jyutakuクラスを定義\n",
    "# jyutaku = ScratchLinearRegression(n_iteration=100,alfa = 0.1)\n",
    "\n",
    "# jyutaku.mse = MSE(y_pred, y)\n",
    "# self.loss_func = Loss_function\n",
    "# print(loss_func)\n",
    "# print(\"loss_func:{}\".format(loss_func))\n",
    "\n",
    "# # #jyutaku.predict(X)   \n",
    "# # y_hat_test = predict(self, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
