{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "###################\n",
    "#インポート\n",
    "###################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import math\n",
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(12000, 28, 28)\n",
      "(60000,)\n",
      "(12000, 10)\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "#データセットの用意\n",
    "###################\n",
    "\n",
    "\n",
    "# データ読み込み\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 正規化\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# onehotベクトル化\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "# 訓練データと評価データに\n",
    "X_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "###################\n",
    "#学習に使用するその他クラスの定義\n",
    "###################\n",
    "\n",
    "class FC:\n",
    "    \"\"\"\n",
    "    Fully connected layers from number of nodes n_nodes1 to n_nodes2\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      Number of nodes in the previous layer\n",
    "    n_nodes2 : int\n",
    "      Number of nodes in subsequent layers\n",
    "    initializer : Instances of initialization methods\n",
    "    optimizer : Instances of optimization methods\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer, activation):\n",
    "\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation\n",
    "        # Initialize.\n",
    "        # Use the initializer method to initialize self.W and self.B\n",
    "        self.W = self.initializer.W(self.n_nodes1,self.n_nodes2)\n",
    "        self.B = self.initializer.B(self.n_nodes2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of the following form, shape (batch_size, n_nodes1)\n",
    "            Input\n",
    "        Returns\n",
    "        ----------\n",
    "        A : ndarray of the following form, shape (batch_size, n_nodes2)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.A = np.dot(self.X,self.W) + self.B\n",
    "\n",
    "        return self.activation.forward(self.A)\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        Backward\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray of the following form, shape (batch_size, n_nodes2)\n",
    "            The gradient flowed in from behind.\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : ndarray of the following form, shape (batch_size, n_nodes1)\n",
    "            forward slope\n",
    "        \"\"\"\n",
    "        dA = self.activation.backward(dZ)\n",
    "        self.dB = np.mean(dA,axis=0)\n",
    "        self.dW = np.dot(self.X.T,dA)/len(self.X)\n",
    "        dZ = np.dot(dA,self.W.T)\n",
    "\n",
    "        # Update\n",
    "        self = self.optimizer.update(self)\n",
    "\n",
    "        return dZ\n",
    "\n",
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    Simple initialization with Gaussian distribution\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      Standard deviation of Gaussian distribution\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        Initializing weights\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          Number of nodes in the previous layer\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in subsequent layers\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : weight\n",
    "        \"\"\"\n",
    "        return self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Bias initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in subsequent layers\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : bias\n",
    "        \"\"\"\n",
    "        return np.zeros(n_nodes2)\n",
    "\n",
    "class HeInitializer():\n",
    "    \"\"\"\n",
    "    Initialization of weights by He\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        Initializing weights\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          Number of nodes in the previous layer\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in subsequent layers\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : weight\n",
    "        \"\"\"\n",
    "        return np.random.randn(n_nodes1, n_nodes2)*np.sqrt(2/n_nodes1)\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Bias initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in subsequent layers\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : bias\n",
    "        \"\"\"\n",
    "        return np.zeros(n_nodes2)\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    stochastic gradient descent method\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : learning rate\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Updating the weights and biases of a layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : An instance of the layer before the update\n",
    "        \"\"\"\n",
    "        layer.W -= self.lr*layer.dW\n",
    "        layer.B -= self.lr*layer.dB\n",
    "\n",
    "        return layer\n",
    "\n",
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    stochastic gradient descent method\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : learning rate\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.hW = 0\n",
    "        self.hB = 0\n",
    "\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Updating the weights and biases of a layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : An instance of the layer before the update\n",
    "        \"\"\"\n",
    "        self.hW += layer.dW*layer.dW\n",
    "        self.hB = layer.dB*layer.dB\n",
    "\n",
    "        layer.W -= self.lr*layer.dW/(np.sqrt(self.hW) +1e-7)\n",
    "        layer.B -= self.lr*layer.dB/(np.sqrt(self.hB) +1e-7)\n",
    "\n",
    "        return layer\n",
    "\n",
    "class ReLU():\n",
    "    \"\"\"\n",
    "    Activation function : ReLU function\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self,A):\n",
    "        self.A = A\n",
    "        return np.maximum(self.A,0)\n",
    "\n",
    "    def backward(self,dZ):\n",
    "\n",
    "        return np.where(self.A>0,dZ,0)\n",
    "\n",
    "class Softmax():\n",
    "    \"\"\"\n",
    "    Activation Function : Softmax Function\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self,A):\n",
    "\n",
    "        return np.exp(A-np.max(A))/np.sum(np.exp(A-np.max(A)),axis=1,keepdims=True)\n",
    "\n",
    "    def backward(self,dZ):\n",
    "        return dZ\n",
    "\n",
    "# Mini-batch processing class\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    Iterator to get the mini-batch\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray of the following form, shape (n_samples, n_features)\n",
    "      Training data\n",
    "    y : ndarray of the following form, shape (n_samples, 1)\n",
    "      correct value\n",
    "    batch_size : int\n",
    "      Batch size\n",
    "    seed : int\n",
    "      Seeding random numbers in NumPy\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=None):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class SimpleInitializerConv2d:\n",
    "    \"\"\"重みとバイアスの初期化（畳込み用）\"\"\"\n",
    "    def __init__(self, sigma=0.01):\n",
    "        \"\"\"コンストラクタ\n",
    "        Parameters\n",
    "        --------------\n",
    "        sigma : ガウス分布の標準偏差\n",
    "        \"\"\"\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def W(self, F, C, FH, FW):\n",
    "        \"\"\"重み初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        F : フィルタ数\n",
    "        C : チャンネル数\n",
    "        FH : フィルターの高さ\n",
    "        FW : フィルターの横幅\n",
    "        \"\"\"\n",
    "        return self.sigma * np.random.randn(F,C,FH,FW)\n",
    "\n",
    "    def B(self, F):\n",
    "        \"\"\"バイアス初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        F : フィルタ数\n",
    "        \"\"\"\n",
    "        return np.zeros(F)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "###################\n",
    "#【問題1】2次元畳み込み層の作成\n",
    "###################\n",
    "\n",
    "class SimpleConv2d():\n",
    "    \"\"\"2次元畳み込みレイヤ\"\"\"\n",
    "    def __init__(self, F, C, FH, FW, P, S,\n",
    "                 initializer=None,optimizer=None,activation=None):\n",
    "        \"\"\"コンストラクタ\n",
    "        Parameters\n",
    "        -------------\n",
    "        F : フィルタ数\n",
    "        C : チャンネル数\n",
    "        FH : フィルターの高さ\n",
    "        FW : フィルターの横幅\n",
    "        P : パディング数\n",
    "        S : ストライド数\n",
    "        initializer : 初期化\n",
    "        optimizer : 最適化手法\n",
    "        activation : 活性化関数\n",
    "        \"\"\"\n",
    "        self.P = P\n",
    "        self.S = S\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation\n",
    "        # 重みとバイアスの初期化\n",
    "        self.W = self.initializer.W(F,C,FH,FW)\n",
    "        self.B = self.initializer.B(F)\n",
    "\n",
    "    def output_shape2d(self,H,W,PH,PW,FH,FW,SH,SW):\n",
    "        \"\"\"出力サイズ計算\n",
    "        H : 入力配列の高さ\n",
    "        W : 入力配列の横幅\n",
    "        FH : フィルターの高さ\n",
    "        FW : フィルターの横幅\n",
    "        PH : パディング数（縦）\n",
    "        PW : パディング数（横）\n",
    "        SH : ストライド数（縦）\n",
    "        SW : ストライド数（横）\n",
    "        \"\"\"\n",
    "        # 高さ計算\n",
    "        OH = (H +2*PH -FH)/SH +1\n",
    "        # 横幅計算\n",
    "        OW = (W +2*PW -FW)/SW +1\n",
    "\n",
    "        return int(OH),int(OW)\n",
    "\n",
    "    def forward(self, X,debug=False):\n",
    "        \"\"\"順伝播\n",
    "        Parameters\n",
    "        ------------\n",
    "        X : 入力配列\n",
    "        \"\"\"\n",
    "        # Xをメンバ変数化\n",
    "        self.X = X\n",
    "        # 入力配列と重みの大きさ取得\n",
    "        N,C,H,W = self.X.shape\n",
    "        F,C,FH,FW = self.W.shape\n",
    "        # 出力サイズ計算\n",
    "        OH,OW = self.output_shape2d(H,W,self.P,self.P,FH,FW,self.S,self.S)\n",
    "        # 各種サイズをメンバ変数化\n",
    "        self.params = N,C,H,W,F,FH,FW,OH,OW\n",
    "        # 返り値の初期化（これを上書きしていく）\n",
    "        A = np.zeros([N,F,OH,OW])\n",
    "        # 計算のためパディング処理\n",
    "        self.X_pad = np.pad(self.X,((0,0),(0,0),(self.P,self.P),(self.P,self.P)))\n",
    "\n",
    "        # バッチ数でループ\n",
    "        for n in range(N):\n",
    "            # フィルター数でループ\n",
    "            for ch in range(F):\n",
    "                # 高さでループ（ストライドを考慮）\n",
    "                for row in range(0,H,self.S):\n",
    "                    # 横幅でループ（ストライドを考慮）\n",
    "                    for col in range(0,W,self.S):\n",
    "                        if self.P == 0 and (W-2 <= col or H-2<=row):\n",
    "                            continue\n",
    "                        # 各要素計算\n",
    "                        A[n,ch,row,col] = \\\n",
    "                        np.sum(self.X_pad[n,:,row:row+FH,col:col+FW]\n",
    "                               *self.W[ch,:,:,:]) \\\n",
    "                        +self.B[ch]\n",
    "        # 活性化関数に通して返す\n",
    "        if debug==True:\n",
    "            return A\n",
    "        else:\n",
    "            return  self.activation.forward(A)\n",
    "\n",
    "    def backward(self, dZ,debug=False):\n",
    "        \"\"\"逆伝播\n",
    "        Parameters\n",
    "        -----------\n",
    "        dZ : 逆伝播してきた値\n",
    "        \"\"\"\n",
    "        # 活性化関数の逆伝播処理\n",
    "        if debug==True:\n",
    "            dA = dZ\n",
    "        else:\n",
    "            dA = self.activation.backward(dZ)\n",
    "        # 順伝播の際にメンバ変数化しておいた各種サイズを取得\n",
    "        N,C,H,W,F,FH,FW,OH,OW = self.params\n",
    "        # 返り値と重みとバイアスの初期化（これを上書きしていく）\n",
    "        dZ = np.zeros(self.X_pad.shape) # X_padのサイズで初期化していることに注意\n",
    "        self.dW = np.zeros(self.W.shape)\n",
    "        self.dB = np.zeros(self.B.shape)\n",
    "\n",
    "        # dZ（逆伝播）\n",
    "        # バッチ数でループ\n",
    "        for n in range(N):\n",
    "            # フィルター数でループ\n",
    "            for ch in range(F):\n",
    "                # 高さでループ（ストライドを考慮）\n",
    "                for row in range(0,H,self.S):\n",
    "                    # 横幅でループ（ストライドを考慮）\n",
    "                    for col in range(0,W,self.S):\n",
    "                        if self.P == 0 and (W-2 <= col or H-2<=row):\n",
    "                            continue\n",
    "                        # 各要素計算\n",
    "                        dZ[n,:,row:row+FH,col:col+FW] += dA[n,ch,row,col]*self.W[ch,:,:,:]\n",
    "\n",
    "        # X_padのサイズになっているので、不要な部分を削除\n",
    "        if self.P == 0:\n",
    "            dZ = np.delete(dZ,[0,H-1],axis=2)\n",
    "            dZ = np.delete(dZ,[0,W-1],axis=3)\n",
    "        else:\n",
    "            dl_rows = range(self.P),range(H+self.P,H+2*self.P,1)\n",
    "            dl_cols = range(self.P),range(W+self.P,W+2*self.P,1)\n",
    "            dZ = np.delete(dZ,dl_rows,axis=2)\n",
    "            dZ = np.delete(dZ,dl_cols,axis=3)\n",
    "\n",
    "        # dW（重み）\n",
    "        # バッチ数でループ\n",
    "        for n in range(N):\n",
    "            # フィルター数でループ\n",
    "            for ch in range(F):\n",
    "                # 高さでループ\n",
    "                for row in range(OH):\n",
    "                    # 横幅でループ\n",
    "                    for col in range(OW):\n",
    "                        # 各要素計算\n",
    "                        self.dW[ch,:,:,:] += dA[n,ch,row,col]*self.X_pad[n,:,row:row+FH,col:col+FW]\n",
    "\n",
    "        # dB（バイアス）\n",
    "        # フィルター数でループ\n",
    "        for ch in range(F):\n",
    "            self.dB[ch] = np.sum(dA[:,ch,:,:])\n",
    "\n",
    "        # 重み更新\n",
    "        self = self.optimizer.update(self)\n",
    "\n",
    "        return dZ\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-4. -4.]\n",
      "   [-4. -4.]]\n",
      "\n",
      "  [[ 1.  1.]\n",
      "   [ 1.  1.]]]]\n",
      "[[[[-5.  4.]\n",
      "   [13. 27.]]]]\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "#【問題2】小さな配列での2次元畳み込み層の実験\n",
    "###################\n",
    "\n",
    "\n",
    "# データの定義\n",
    "x = np.array([[[[ 1,  2,  3,  4],[ 5,  6,  7,  8],[ 9, 10, 11, 12],[13, 14, 15, 16]]]])\n",
    "w = np.array([[[[ 0.,  0.,  0.],[ 0.,  1.,  0.],[ 0., -1.,  0.]]],[[[ 0.,  0.,  0.],[ 0., -1.,  1.],[ 0.,  0.,  0.]]]])\n",
    "da = np.array([[[[ -4,  -4], [ 10,  11]],[[  1,  -7],[  1, -11]]]])\n",
    "\n",
    "# インスタンス定義と重み上書き\n",
    "simple_conv_2d = SimpleConv2d(F=2, C=1, FH=3, FW=3, P=0, S=1,initializer=SimpleInitializerConv2d(),optimizer=SGD(),activation=ReLU())\n",
    "simple_conv_2d.W = w\n",
    "\n",
    "# 順伝播\n",
    "A = simple_conv_2d.forward(x,True)\n",
    "print(A)\n",
    "\n",
    "# 逆伝播\n",
    "dZ = simple_conv_2d.backward(da,True)\n",
    "print(dZ)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4, 28, 28)\n",
      "(5, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "### mnistを使ったテスト\n",
    "\n",
    "# ストライド/パディング/バッチ数の定義\n",
    "S = 1\n",
    "P = 0\n",
    "N = 5\n",
    "\n",
    "# 入力配列と重みのサイズ定義\n",
    "N,C,H,W = (5,1,28,28)\n",
    "F,C,FH,FW = (4,1,3,3)\n",
    "\n",
    "# X（入力配列生成）\n",
    "X_sample = X_train[0:N].reshape(N,C,H,W)\n",
    "\n",
    "# インスタンス生成\n",
    "simple_conv_2d = SimpleConv2d(F=4, C=1, FH=3, FW=3, P=1, S=1,initializer=SimpleInitializerConv2d(),optimizer=SGD(),activation=ReLU())\n",
    "\n",
    "# 順伝播\n",
    "A = simple_conv_2d.forward(X_sample)\n",
    "print(A.shape)\n",
    "\n",
    "# 逆伝播\n",
    "dA = np.ones(A.shape)\n",
    "dZ = simple_conv_2d.backward(X_sample)\n",
    "print(dZ.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "(4, 4)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################\n",
    "#【問題3】2次元畳み込み後の出力サイズ\n",
    "###################\n",
    "\n",
    "simple_conv_2d.output_shape2d(H=6,W=6,PH=0,PW=0,FH=3,FW=3,SH=1,SW=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "###################\n",
    "#【問題4】最大プーリング層の作成\n",
    "###################\n",
    "\n",
    "\n",
    "class MaxPool2D():\n",
    "    \"\"\"最大プーリング層\n",
    "    \"\"\"\n",
    "    def __init__(self,P):\n",
    "        \"\"\"コンストラクタ\n",
    "        Parameters\n",
    "        -----------\n",
    "        P : プーリング幅\n",
    "        \"\"\"\n",
    "        self.P = P\n",
    "        # 順伝播の返り値\n",
    "        self.PA = None\n",
    "        # 最大値のインデックス記録\n",
    "        self.Pindex = None\n",
    "\n",
    "    def forward(self,A):\n",
    "        \"\"\"順伝播\n",
    "        Parameters\n",
    "        -----------\n",
    "        A : 入力配列\n",
    "        \"\"\"\n",
    "        # 入力配列のサイズ\n",
    "        N,F,OH,OW = A.shape\n",
    "        #\n",
    "        PS = self.P\n",
    "        # 縦軸と横軸のスライド回数\n",
    "        PH,PW = int(OH/PS),int(OW/PS)\n",
    "\n",
    "        # 各種パラメータの保存\n",
    "        self.params = N,F,OH,OW,PS,PH,PW\n",
    "\n",
    "        # プーリング処理のための初期化\n",
    "        self.PA = np.zeros([N,F,PH,PW])\n",
    "        self.Pindex = np.zeros([N,F,PH,PW])\n",
    "\n",
    "        # バッチ数でループ\n",
    "        for n in range(N):\n",
    "            # フィルター数でループ\n",
    "            for ch in range(F):\n",
    "                # 縦方向スライド回数\n",
    "                for row in range(PH):\n",
    "                    # 横方向スライド回数\n",
    "                    for col in range(PW):\n",
    "                        # 順伝播の値計算\n",
    "                        self.PA[n,ch,row,col] = \\\n",
    "                        np.max(A[n,ch,row*PS:row*PS+PS,col*PS:col*PS+PS])\n",
    "                        # 最大値のインデックス記録\n",
    "                        self.Pindex[n,ch,row,col] = \\\n",
    "                        np.argmax(A[n,ch,row*PS:row*PS+PS,col*PS:col*PS+PS])\n",
    "\n",
    "        return self.PA\n",
    "\n",
    "    def backward(self,dA):\n",
    "        \"\"\"逆伝播の値\n",
    "        Parameters\n",
    "        -----------\n",
    "        dA : 逆伝播してきた値\n",
    "        \"\"\"\n",
    "        # 保存しておいた各種パラメータ取得\n",
    "        N,F,OH,OW,PS,PH,PW = self.params\n",
    "        # 逆伝播の値\n",
    "        dP = np.zeros([N,F,OH,OW])\n",
    "        # バッチ数でループ\n",
    "        for n in range(N):\n",
    "            # フィルター数でループ\n",
    "            for ch in range(F):\n",
    "                # 縦方向スライド回数\n",
    "                for row in range(PH):\n",
    "                    # 横方向スライド回数\n",
    "                    for col in range(PW):\n",
    "                        # 最大値を取得してきたインデックスの取得\n",
    "                        idx = self.Pindex[n,ch,row,col]\n",
    "                        # 逆伝播の一時保存変数\n",
    "                        tmp = np.zeros((PS*PS))\n",
    "                        for i in range(PS*PS):\n",
    "                            # 該当インデックスはその値\n",
    "                            if i == idx:\n",
    "                                tmp[i] = dA[n,ch,row,col]\n",
    "                            # それ以外は0\n",
    "                            else:\n",
    "                                tmp[i] = 0\n",
    "                        # 返り値の該当場所に格納\n",
    "                        dP[n,ch,row*PS:row*PS+PS,col*PS:col*PS+PS] = tmp.reshape(PS,PS)\n",
    "\n",
    "        return dP"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------X\n",
      "[[[[4 7 0 3 4 3]\n",
      "   [7 3 8 1 2 3]\n",
      "   [1 6 0 8 5 3]\n",
      "   [7 8 3 2 8 0]\n",
      "   [6 1 2 4 1 7]\n",
      "   [6 1 3 2 7 3]]]]\n",
      "---------------A\n",
      "[[[[7. 8. 4.]\n",
      "   [8. 8. 8.]\n",
      "   [6. 4. 7.]]]]\n",
      "---------------dA\n",
      "[[[[6 7 4]\n",
      "   [8 7 1]\n",
      "   [5 5 0]]]]\n",
      "---------------dZ\n",
      "[[[[0. 6. 0. 0. 4. 0.]\n",
      "   [0. 0. 7. 0. 0. 0.]\n",
      "   [0. 0. 0. 7. 0. 0.]\n",
      "   [0. 8. 0. 0. 1. 0.]\n",
      "   [5. 0. 0. 5. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0.]]]]\n"
     ]
    }
   ],
   "source": [
    "# テスト\n",
    "# データ準備\n",
    "X = np.random.randint(0,9,36).reshape(1,1,6,6)\n",
    "print(\"---------------X\")\n",
    "print(X)\n",
    "\n",
    "# インスタンス生成と順伝播\n",
    "Pooling = MaxPool2D(P=2)\n",
    "A = Pooling.forward(X)\n",
    "print(\"---------------A\")\n",
    "print(A)\n",
    "\n",
    "# 逆伝播してきた配列定義\n",
    "dA = np.random.randint(0,9,9).reshape(A.shape)\n",
    "print(\"---------------dA\")\n",
    "print(dA)\n",
    "\n",
    "# 逆伝播\n",
    "dZ = Pooling.backward(dA)\n",
    "print(\"---------------dZ\")\n",
    "print(dZ)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "###################\n",
    "#【問題6】平滑化\n",
    "###################\n",
    "\n",
    "\n",
    "class Flatten:\n",
    "    \"\"\"平滑化レイヤー\"\"\"\n",
    "    def __ini__(self):\n",
    "        \"\"\"コンストラクタ\"\"\"\n",
    "        pass\n",
    "    def forward(self,X):\n",
    "        \"\"\"順伝播\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : 入力配列\n",
    "        \"\"\"\n",
    "        self.shape = X.shape\n",
    "        return X.reshape(len(X),-1)\n",
    "\n",
    "    def backward(self,X):\n",
    "        \"\"\"逆伝播の値\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : 逆伝播してきた値\n",
    "        \"\"\"\n",
    "        return X.reshape(self.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward_shape: (20, 50)\n",
      "Backward_shape: (20, 2, 5, 5)\n"
     ]
    }
   ],
   "source": [
    "# テスト\n",
    "# データ準備\n",
    "TEST = np.zeros([20,2,5,5])\n",
    "\n",
    "# インスタンス生成\n",
    "flt = Flatten()\n",
    "\n",
    "# 順伝播\n",
    "flat_forward = flt.forward(TEST)\n",
    "\n",
    "# 逆伝播\n",
    "flat_back = flt.backward(flat_forward)\n",
    "\n",
    "print('Forward_shape:',flat_forward.shape)\n",
    "print('Backward_shape:',flat_back.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "###################\n",
    "#【問題7】学習と推定\n",
    "###################\n",
    "\n",
    "\n",
    "# Scratch CNN\n",
    "class Scratch2dCNNClassifier():\n",
    "    \"\"\"CNNスクラッチ\n",
    "    \"\"\"\n",
    "    def __init__(self, NN, CNN, n_epoch=5, n_batch=1, verbose = False):\n",
    "        \"\"\"コンストラクタ\n",
    "        Parameters\n",
    "        -----------\n",
    "        NN : 辞書型でレイヤーのインスタンスを格納\n",
    "        CNN : 辞書型でレイヤーのインスタンスを格納\n",
    "        n_epoch : 学習回数\n",
    "        n_batch : バッチ数\n",
    "        verbose : ログ出力するか否か\n",
    "        \"\"\"\n",
    "        self.NN = NN\n",
    "        self.CNN = CNN\n",
    "        self.n_epoch = n_epoch\n",
    "        self.n_batch = n_batch\n",
    "        self.verbose = verbose\n",
    "        # ログ記録用\n",
    "        self.log_loss = np.zeros(self.n_epoch)\n",
    "        self.log_acc = np.zeros(self.n_epoch)\n",
    "\n",
    "\n",
    "    def loss_function(self,y,yt):\n",
    "        \"\"\"クロスエントロピー誤差\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : 予測値\n",
    "        yt : 正解データ\n",
    "        \"\"\"\n",
    "        delta = 1e-7\n",
    "        return -np.mean(yt*np.log(y+delta))\n",
    "\n",
    "    def accuracy(self,Z,Y):\n",
    "        \"\"\"クロスエントロピー誤差\n",
    "        Parameters\n",
    "        ----------\n",
    "        Z : 予測値\n",
    "        Y : 正解データ\n",
    "        \"\"\"\n",
    "        return accuracy_score(Y,Z)\n",
    "\n",
    "    def fit(self, X, y, X_val=False, y_val=False):\n",
    "        \"\"\"学習\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 訓練データの説明変数\n",
    "        y : 訓練データの目的変数\n",
    "        X_val : 評価データの説明変数\n",
    "        y_val : 評価データの目的変数\n",
    "        \"\"\"\n",
    "        # 学習回数分ループ\n",
    "        for epoch in range(self.n_epoch):\n",
    "            # ミニバッチイテレータ生成\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.n_batch)\n",
    "            # バッチの合計損失格納\n",
    "            self.loss = 0\n",
    "            # ミニバッチイテレータでループ\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                ############### 順伝播\n",
    "                # データ準備\n",
    "                forward_data = mini_X_train[:,np.newaxis,:,:]\n",
    "                # 畳込み\n",
    "                for layer in range(len(self.CNN)):\n",
    "                    forward_data = self.CNN[layer].forward(forward_data)\n",
    "                # 平滑化\n",
    "                flt = Flatten()\n",
    "                forward_data = flt.forward(forward_data)\n",
    "                # 通常のNN\n",
    "                for layer in range(len(self.NN)):\n",
    "                    forward_data = self.NN[layer].forward(forward_data)\n",
    "\n",
    "                ############### 逆伝播\n",
    "                # データ準備\n",
    "                Z = forward_data\n",
    "                backward_data = (Z - mini_y_train)/self.n_batch\n",
    "                # 通常のNN\n",
    "                for layer in range(len(self.NN)-1,-1,-1):\n",
    "                    backward_data = self.NN[layer].backward(backward_data)\n",
    "                # 平滑化\n",
    "                backward_data = flt.backward(backward_data)\n",
    "                # 畳み込み\n",
    "                for layer in range(len(self.CNN)-1,-1,-1):\n",
    "                    backward_data = self.CNN[layer].backward(backward_data)\n",
    "\n",
    "                # 損失計算\n",
    "                self.loss += self.loss_function(Z,mini_y_train)\n",
    "\n",
    "            if verbose:\n",
    "                print(self.loss/len(get_mini_batch),self.accuracy(self.predict(X),np.argmax(y,axis=1)))\n",
    "            # 損失記録用\n",
    "            self.log_loss[epoch] = self.loss/len(get_mini_batch)\n",
    "            self.log_acc[epoch] = self.accuracy(self.predict(X),np.argmax(y,axis=1))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"予測\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 説明変数\n",
    "        \"\"\"\n",
    "        # データ準備\n",
    "        pred_data = X[:,np.newaxis,:,:]\n",
    "\n",
    "        # 畳込み\n",
    "        for layer in range(len(self.CNN)):\n",
    "            pred_data = self.CNN[layer].forward(pred_data)\n",
    "        # 平滑化\n",
    "        pred_data = flt.forward(pred_data)\n",
    "        # 通常のNN\n",
    "        for layer in range(len(self.NN)):\n",
    "            pred_data = self.NN[layer].forward(pred_data)\n",
    "        # 最も大きい値のインデックスを採用\n",
    "        return np.argmax(pred_data,axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# レイヤー群定義\n",
    "NN = {\n",
    "    0:FC(6760, 400, HeInitializer(), AdaGrad(0.01), ReLU()),\n",
    "    1:FC(400, 200, HeInitializer(), AdaGrad(0.01), ReLU()),\n",
    "    2:FC(200, 10, SimpleInitializer(0.01), AdaGrad(0.01), Softmax()),\n",
    "}\n",
    "\n",
    "CNN = {\n",
    "    0:SimpleConv2d(\n",
    "        F=10, C=1, FH=3, FW=3, P=0, S=1,\n",
    "        initializer=SimpleInitializerConv2d(),\n",
    "        optimizer=SGD(),\n",
    "        activation=ReLU())\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# CNNクラスのインスタンス化\n",
    "cnn1 = Scratch2dCNNClassifier(NN=NN,CNN=CNN,n_epoch=10,n_batch=200,verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'verbose' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-44-bb91121ca417>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# 学習\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mcnn1\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;36m1000\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0my_train_one_hot\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;36m1000\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-41-2e74b037c467>\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, X, y, X_val, y_val)\u001B[0m\n\u001B[0;32m     93\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloss\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloss_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mZ\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mmini_y_train\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     94\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 95\u001B[1;33m             \u001B[1;32mif\u001B[0m \u001B[0mverbose\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     96\u001B[0m                 \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mget_mini_batch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maccuracy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0margmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     97\u001B[0m             \u001B[1;31m# 損失記録用\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'verbose' is not defined"
     ]
    }
   ],
   "source": [
    "# 学習\n",
    "cnn1.fit(X_train[0:1000],y_train_one_hot[0:1000])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 予測\n",
    "y_pred = cnn1.predict(X_val[0:100])\n",
    "\n",
    "# ACC算出\n",
    "accuracy = accuracy_score(np.argmax(y_val[0:100],axis=1), y_pred)\n",
    "print('accuracy:{:.3f}'.format(accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 学習曲線の可視化\n",
    "plt.rcParams[\"font.size\"] = 20\n",
    "fig=plt.subplots(figsize=(16,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('LOSS')\n",
    "plt.plot(cnn1.log_loss,'bo--')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('ACC')\n",
    "plt.plot(cnn1.log_acc,'rs--');\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###################\n",
    "#【問題10】出力サイズとパラメータ数の計算\n",
    "###################\n",
    "\n",
    "\n",
    "1.\n",
    "\n",
    "* 入力サイズ : 144×144, 3チャンネル\n",
    "* フィルタサイズ : 3×3, 6チャンネル\n",
    "* ストライド : 1\n",
    "* パディング : なし\n",
    "\n",
    "→ 出力サイズ：6×142×142\n",
    "\n",
    "→ パラメータ数（重み）（F×C×FH×FW）：162\n",
    "\n",
    "→ パラメータ数（バイアス）（F）：6\n",
    "\n",
    "2.\n",
    "\n",
    "* 入力サイズ : 60×60, 24チャンネル\n",
    "* フィルタサイズ : 3×3, 48チャンネル\n",
    "* ストライド　: 1\n",
    "* パディング : なし\n",
    "\n",
    "→ 出力サイズ：48×58×58\n",
    "\n",
    "→ パラメータ数（重み）（F×C×FH×FW）：10368\n",
    "\n",
    "→ パラメータ数（バイアス）（F）：48\n",
    "\n",
    "3.\n",
    "\n",
    "* 入力サイズ : 20×20, 10チャンネル\n",
    "* フィルタサイズ: 3×3, 20チャンネル\n",
    "* ストライド : 2\n",
    "* パディング : なし\n",
    "\n",
    "→ 出力サイズ：20x9x9\n",
    "\n",
    "→ パラメータ数（重み）（F×C×FH×FW）：1800\n",
    "\n",
    "→ パラメータ数（バイアス）（F）：20\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}